row 436 #P: we already did this at row 421-426
row 550 #P: check in 2Class Setting only first two records are used BY DEFAULT
row 637 #P: we can remove the redundancy then?
row 636 #P: yes but SOME of this data are used before for training/test/validate, right? 
row 738 #P: just for confirmation, the trainFeatVSVM is made only by VSVs?
roe 770 #P: we should keep the Random, but for the sake of balanced train set we use Balanced
row 36  trainControl -> how does actually work with the indexTrain
row 197 rbf_kernel -> that I added is required, right?
row 371 abs(pred) -> for the distance we need only positive values, right? (I added abs)
row 603 ref_added_or[1:250,]$distance = 1 -> which are the actual values for the distance? after norm, the max value 1
row 956 binaryClassProblem 

Inside svm package, the difference between probability class and distance class -> see both related papers and script function

USE BINARY CLASIFFICATION TO TEST THE SCRIPT
CHECK THE VALUES INSIDE THE COMPUTED VECTORS

Lu et al.(2016): A Novel Synergetic Classification Approach for  Hyperspectral and Panchromatic Images Based on Self-Learning

how to improve performance 
DONE check how many CPU cores are present on the RSRG server and how to use them
DONE vectorize: apply() {R function} + predict() {kernlab function} 

********************************************************
AAA  implement MS-cSV
DONE plots for different model accuracies with different training data size and unlabeled samples

 1.  DONE try without indexTrain  
 2.  perform PCA / t-SNE for visualization and clustering
 3.  DONE split alter_samples in multiple iterations 
 4.  DONE check if in the multiclas setting the random / balanced unlabeled samples actually change
 5.  DONE implement only_probability_distance + check different implementation of mclu (see paper) 
 6.  DONE check if NDVI feature is actually useful
 7.  focus on meadow and other_impervious_surface classes (the classes that ends up with less labels) add new sample from/for the worse performing class
 8.  DONE implement binary + multiclass in the same script
 9.  DONE tune alter_label hyperparameter: how many label need to be relabeled?
 10. DONE overall hyperparameters optimization e.g. "boundMargin" and "bound" 
 11. DONE descriptive stats of the dataset/data visualization
 12. Kappa-score coefficient definition: change the metric from "kappa" to "ROC" (and maybe also to "accuracy") 
 13. check why if(!tmp_cond){VSV1[k,]=NA} in rem_extrem_kerneldist doesn't work
 14. one vs all VSM Classification instead of binarymulticlass
 15. DONE why multiclass script is implemented differently from the binary ones and got different accuracies
 16. add a flag to see which label actually changed
 17. DONE add shape scripts
 18. add hadagera scripts
 19. DONE check if it is sufficient to run registerDoParallel(num_cores) just once inside the script
 20. DONE either balanced the datapool at the beginning or the each train/test/validate/unlabeled set
 21. DONE pick one new sample at time OR pick multiple from different regions/classes
 22. change svm method 
 23. change from svm to ksvm
 24. DONE implement VSVM-SL + VIRTUAL Unlabeled Samples on new_tunedVSVM IT
 25. DONE use the kernel_function from the base_svm 
 26. try to predict normalized_data -> onlz for visualiyation purposes (thematic map?)
 27. compare ud accuracies with and without clustering
 28. with few initial samples, check how many samples actuallz are present when we train each model
 29. compare VSVM / VSVM_SL / VSVM_SL Unl / VSVM_SL V Unl as base model for ITerative AL and especially VSVM_SL Unl
 30. check why SVM as an Accuracy of 90% just with 3 samples
 31. check sampleSize = sampleSizePor[sample_size] - sampleSizePor[sample_size-1]
 32. compare VSVM_SL_Un_it trained on SVM/VSVM with VSVM_SL_Un_b/VSVM_SL_vUn_b
 33. compare acc from VSVM_SL_Un_it trained before and after VSVM_SL_vUn_b vs VSVM_SL_vUn_b trained before and after VSVM_SL_Un_it
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#from dfply import arrange\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/home/rsrg9/Documents/tunc_oz/apply_model\"\n",
    "#path1 = \"D:/tunc_oz/apply_model\"\n",
    "os.chdir(path1)\n",
    "\n",
    "# Second path\n",
    "path2 = \"csv_data_r_import/cologne/scale\"\n",
    "os.chdir(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.97333333])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X.shape, y.shape\n",
    "\n",
    "def custom_cv_2folds(X):\n",
    "    n = X.shape[0]\n",
    "    i = 1\n",
    "    while i <= 2:\n",
    "        idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n",
    "        yield idx, idx\n",
    "        i += 1\n",
    "custom_cv = custom_cv_2folds(X)\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "cross_val_score(clf, X, y, cv=custom_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object custom_cv_2folds at 0x000001E977252970>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(x, y, custom_splitter=None):\n",
    "    # Expand coarse grid\n",
    "    coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                   'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "    \n",
    "    kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "    # Coarse grid search\n",
    "    svm_coarse = SVC(kernel='rbf')\n",
    "    svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring=kappa_scorer, cv=custom_splitter)\n",
    "    svm_coarse_cv.fit(x, y)\n",
    "    \n",
    "    # Get best coarse grid parameters\n",
    "    best_c = svm_coarse_cv.best_params_['C']\n",
    "    best_gamma = svm_coarse_cv.best_params_['gamma']\n",
    "    \n",
    "    # Define narrow grid borders\n",
    "    a_gamma = np.log2(best_gamma) - 2\n",
    "    b_gamma = np.log2(best_gamma) + 2\n",
    "    a_c = np.log2(best_c) - 2\n",
    "    b_c = np.log2(best_c) + 2\n",
    "    \n",
    "    # Expand narrow grid\n",
    "    narrow_grid = {'C': 2.0 ** np.arange(a_c, b_c, 0.5),\n",
    "                   'gamma': 2.0 ** np.arange(a_gamma, b_gamma, 0.5)}\n",
    "    \n",
    "    # Narrow grid search\n",
    "    svm_narrow = SVC(kernel='rbf')\n",
    "    svm_narrow_cv = GridSearchCV(svm_narrow, param_grid=narrow_grid, scoring=kappa_scorer, cv=custom_splitter)\n",
    "    svm_narrow_cv.fit(x, y)\n",
    "    \n",
    "    return svm_narrow_cv\n",
    "\n",
    "# Usage example:\n",
    "# svm_model = svm_fit(X_train, y_train, StratifiedKFold(n_splits=10, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance between two points lying in the input space\n",
    "def euc_dis(a, b):\n",
    "    temp = 0\n",
    "    for ii in range(len(a)):\n",
    "        temp += (a[ii] - b[ii])**2\n",
    "    return np.sqrt(temp)\n",
    "\n",
    "# Evaluate the distance between Virtual Support Vectors and Support Vectors lying in the input space\n",
    "def rem_extrem(org, VSV1, a):\n",
    "    distance = pd.DataFrame(index=range(len(org)), columns=['label', 'distance'])\n",
    "    distanceSVC1 = []\n",
    "    distanceSVC2 = []\n",
    "    \n",
    "    for l in range(len(org)):\n",
    "        distance.loc[l, 'label'] = str(org.iloc[l, -1])\n",
    "        distance.loc[l, 'distance'] = euc_dis(org.iloc[l, :-1], VSV1.iloc[l, :-1])\n",
    "    \n",
    "    SVClass1 = org[org['REF'] == org['REF'].unique()[0]]\n",
    "    SVClass2 = org[org['REF'] == org['REF'].unique()[1]]\n",
    "    \n",
    "    if len(SVClass1) > 0:\n",
    "        for n in range(len(SVClass1) - 1):\n",
    "            for nn in range(n, len(SVClass1) - 1):\n",
    "                distanceSVC1.append(euc_dis(SVClass1.iloc[n, :-1], SVClass1.iloc[n + nn, :-1]))\n",
    "        disClass1median = np.mean(distanceSVC1)\n",
    "        boundClass1 = disClass1median * a\n",
    "    \n",
    "    if len(SVClass2) > 0:\n",
    "        for n in range(len(SVClass2) - 1):\n",
    "            for nn in range(n, len(SVClass2) - 1):\n",
    "                distanceSVC2.append(euc_dis(SVClass2.iloc[n, :-1], SVClass2.iloc[n + nn, :-1]))\n",
    "        disClass2median = np.mean(distanceSVC2)\n",
    "        boundClass2 = disClass2median * a\n",
    "    \n",
    "    for k in range(len(org)):\n",
    "        if np.isnan(distance.loc[k, 'distance']):\n",
    "            VSV1.iloc[k, :] = np.nan\n",
    "        else:\n",
    "            if boundClass1 is not None:\n",
    "                if distance.loc[k, 'label'] == org['REF'].unique()[0]:\n",
    "                    if distance.loc[k, 'distance'] > boundClass1:\n",
    "                        VSV1.iloc[k, :] = np.nan\n",
    "            else:\n",
    "                if boundClass2 is not None:\n",
    "                    if distance.loc[k, 'label'] == org['REF'].unique()[1]:\n",
    "                        if distance.loc[k, 'distance'] > boundClass2:\n",
    "                            VSV1.iloc[k, :] = np.nan\n",
    "    return VSV1\n",
    "\n",
    "# Kernel distance between two points lying in the hyperspace\n",
    "def kern_dis(a, b, kernelfunc):\n",
    "    a = np.array(a).flatten()\n",
    "    b = np.array(b).flatten()\n",
    "    dk = np.sqrt(kernelfunc(a, a) + kernelfunc(b, b) - 2 * kernelfunc(a, b))\n",
    "    return dk\n",
    "\n",
    "# Evaluate the distance between Virtual Support Vectors and Support Vectors lying in the hyperspace\n",
    "def rem_extrem_kerneldist(org, VSV1, a, kernelfunc):\n",
    "    distance = pd.DataFrame(index=range(len(org)), columns=['label', 'distance'])\n",
    "    distanceSVC1 = []\n",
    "    distanceSVC2 = []\n",
    "    \n",
    "    for l in range(len(org)):\n",
    "        distance.loc[l, 'label'] = str(org.iloc[l, -1])\n",
    "        distance.loc[l, 'distance'] = kern_dis(org.iloc[l, :-1], VSV1.iloc[l, :-1], kernelfunc)\n",
    "    \n",
    "    SVClass1 = org[org['REF'] == org['REF'].unique()[0]]\n",
    "    SVClass2 = org[org['REF'] == org['REF'].unique()[1]]\n",
    "    \n",
    "    if len(SVClass1) > 0:\n",
    "        for n in range(len(SVClass1) - 1):\n",
    "            for nn in range(n, len(SVClass1) - 1):\n",
    "                distanceSVC1.append(kern_dis(SVClass1.iloc[n, :-1], SVClass1.iloc[n + nn, :-1], kernelfunc))\n",
    "        disClass1median = np.mean(distanceSVC1)\n",
    "        boundClass1 = disClass1median * a\n",
    "    \n",
    "    if len(SVClass2) > 0:\n",
    "        for n in range(len(SVClass2) - 1):\n",
    "            for nn in range(n, len(SVClass2) - 1):\n",
    "                distanceSVC2.append(kern_dis(SVClass2.iloc[n, :-1], SVClass2.iloc[n + nn, :-1], kernelfunc))\n",
    "        disClass2median = np.mean(distanceSVC2)\n",
    "        boundClass2 = disClass2median * a\n",
    "    \n",
    "    for k in range(len(org)):\n",
    "        if np.isnan(distance.loc[k, 'distance']):\n",
    "            VSV1.iloc[k, :] = np.nan\n",
    "        else:\n",
    "            if boundClass1 is not None:\n",
    "                if distance.loc[k, 'label'] == org['REF'].unique()[0]:\n",
    "                    if distance.loc[k, 'distance'] > boundClass1:\n",
    "                        VSV1.iloc[k, :] = np.nan\n",
    "            else:\n",
    "                if boundClass2 is not None:\n",
    "                    if distance.loc[k, 'label'] == org['REF'].unique()[1]:\n",
    "                        if distance.loc[k, 'distance'] > boundClass2:\n",
    "                            VSV1.iloc[k, :] = np.nan\n",
    "    return VSV1\n",
    "\n",
    "def pred_one(model, data_point):\n",
    "    # Extract necessary components from the SVM model\n",
    "    support_vectors = model.n_support_\n",
    "    kernel_function = model.kernel\n",
    "    coefficients = model.dual_coef_.ravel()\n",
    "    intercept = model.intercept_\n",
    "    \n",
    "    # Initialize prediction variable\n",
    "    prediction = 0\n",
    "    \n",
    "    # Iterate over each support vector\n",
    "    for j in range(len(support_vectors)):\n",
    "        # Compute kernel function value between the j-th support vector and the data point\n",
    "        kernel_value = kernel_function(data_point.reshape(1, -1), model.support_vectors_[j, :].reshape(1, -1))\n",
    "        \n",
    "        # Multiply kernel value by the corresponding coefficient and add to prediction\n",
    "        weighted_value = kernel_value * coefficients[j]\n",
    "        prediction += weighted_value\n",
    "    \n",
    "    # Subtract intercept to get the final prediction\n",
    "    final_prediction = prediction - intercept\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "def uncertainty_dist_v2_2(org, samp):\n",
    "    distance = pd.DataFrame(columns=['control_label', 'distance'], index=range(len(samp)))\n",
    "    \n",
    "    for k in range(len(samp)):\n",
    "        distance.loc[k, 'distance'] = np.sign(pred_one(org.finalModel, samp.iloc[k, :-1])) * \\\n",
    "                                      np.where(pred_one(org.finalModel, samp.iloc[k, :-1]) > 0, 1, -1)\n",
    "    \n",
    "    # Normalize distance\n",
    "    preProc = preprocessing.MinMaxScaler()\n",
    "    preProc.fit(distance[['distance']])\n",
    "    normdistance = preProc.transform(distance[['distance']])\n",
    "    \n",
    "    samp['normdistance'] = normdistance\n",
    "    \n",
    "    return samp\n",
    "\n",
    "def alter_labels(distance_data, ref):\n",
    "    # Merge features and original labels\n",
    "    ref_added = pd.concat([distance_data, ref], axis=1)\n",
    "    # Order by most uncertain samples\n",
    "    ref_added_or = ref_added.sort_values(by='distance')\n",
    "    # Re-label most uncertain n number of samples\n",
    "    ref_added_or.iloc[:250, -1] = ref_added_or.iloc[:250, -2]\n",
    "    ref_added_or.iloc[:250, -2] = 1.0\n",
    "    # Re-order dataset by its index\n",
    "    ref_added_or['index'] = range(len(ref_added_or))\n",
    "    ref_added_reor = ref_added_or.sort_values(by='index')\n",
    "    \n",
    "    # Extract labels for prediction\n",
    "    labels = ref_added_reor.iloc[:, -5]\n",
    "    return labels\n",
    "\n",
    "def ExCsvMSD(datadase, filename=None):\n",
    "    # Convert to numpy array\n",
    "    datadase = np.array(datadase)\n",
    "    n = datadase.shape[1]\n",
    "    MSDdata = np.empty((2, n), dtype=float)\n",
    "    \n",
    "    MSDdata[0, :] = np.mean(datadase, axis=0)\n",
    "    MSDdata[1, :] = np.std(datadase, axis=0)\n",
    "    \n",
    "    MSDdata_final = np.vstack((datadase, MSDdata))\n",
    "    \n",
    "    # Export final mean and standard deviation to .csv-file\n",
    "    if filename is not None:\n",
    "        pd.DataFrame(MSDdata_final).to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    return MSDdata_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"cologne_res_100_L2-L13.csv\"\n",
    "sMax = 1000\n",
    "bound = [0.3, 0.6, 0.9]\n",
    "boundMargin = [1.5, 1.0, 0.5]\n",
    "sampleSizesPor = [40, 25, 16, 12, 10, 8, 6, 4, 3, 2, 1]\n",
    "colheader = [\"40\", \"25\", \"16\", \"12\", \"10\", \"8\", \"6\", \"4\", \"3\", \"2\", \"1\"]\n",
    "sindexSVMDATA = 36\n",
    "numFeat = 18\n",
    "eindexSVMDATA = sindexSVMDATA + numFeat - 1\n",
    "objInfoNames = [\"Lx_g_comp\", \"Lx_g_elfi\", \"Lx_g_refi\", \"Lx_g_roun\", \"Lx_g_shin\",\n",
    "                \"Lx_m_bl\", \"Lx_m_gr\", \"Lx_m_ndvi\", \"Lx_m_nir\", \"Lx_m_re\",\n",
    "                \"Lx_sd_bl\", \"Lx_sd_gr\", \"Lx_sd_ndvi\", \"Lx_sd_nir\", \"Lx_sd_re\",\n",
    "                \"Lx_t_diss\", \"Lx_t_hom\", \"Lx_t_mean\",\n",
    "                \"label\"]\n",
    "columnClass = [None] * 217 + [\"factor\", \"integer\"]\n",
    "\n",
    "# Import data\n",
    "preproc_DataPool = pd.read_csv(inputPath, header=0, sep=\";\", dtype=str, na_values=None)\n",
    "\n",
    "tmp_DataPool = preproc_DataPool.iloc[:, :-2]\n",
    "\n",
    "generalDataPool_columns = tmp_DataPool.columns\n",
    "\n",
    "converters = {col: lambda x: float(x.replace(',', '.')) for col in generalDataPool_columns}\n",
    "generalDataPool = pd.read_csv(inputPath, header=0, sep=\";\", na_values=None, converters=converters)\n",
    "\n",
    "\n",
    "generalDataPool.dropna(subset=[\"REF\"], inplace=True)  # Remove rows with missing REF values\n",
    "generalDataPool[\"REF\"] = pd.Categorical(generalDataPool[\"REF\"])\n",
    "\n",
    "# Transform to 2-Class-Case \"Bushes Trees\" VS rest\n",
    "first_label_class = generalDataPool[\"REF\"].cat.categories[0]  # Note that the first record is of class \"bushes trees\"\n",
    "generalDataPool[\"REF\"] = generalDataPool[\"REF\"].apply(lambda x: first_label_class if x == first_label_class else \"other\")\n",
    "generalDataPool[\"REF\"] = pd.Categorical(generalDataPool[\"REF\"])\n",
    "\n",
    "data = generalDataPool.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "REF = generalDataPool.iloc[:, -1]\n",
    "data_with_label = pd.concat([data, REF], axis=1)\n",
    "data_label = data_with_label.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalDataPool.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizedFeat['L02_G_COMP'] = normalizedFeat['L02_G_COMP'].replace(',', '.', regex=True)\n",
    "#normalizedFeat['L02_G_COMP'] = normalizedFeat['L02_G_COMP'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizedFeat['L02_G_EFIT'] = normalizedFeat['L02_G_EFIT'].apply(lambda x: pd.to_numeric(x.str.replace(',', '.'), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedFeat = generalDataPool.iloc[:, :-2]\n",
    "normalizedLabelUSE = generalDataPool.iloc[:, -2:]\n",
    "\n",
    "# Scaling\n",
    "preProc = MinMaxScaler()\n",
    "normalizedFeatBase = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]), columns=objInfoNames[:-1])\n",
    "\n",
    "# Apply range of basemodel to all levels\n",
    "normalizedFeat2 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, :numFeat]), columns=objInfoNames[:-1])\n",
    "normalizedFeat3 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, numFeat:(2 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat5 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (3 * numFeat):(4 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat6 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (4 * numFeat):(5 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat7 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (5 * numFeat):(6 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat8 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (6 * numFeat):(7 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat9 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (7 * numFeat):(8 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat10 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (8 * numFeat):(9 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat11 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (9 * numFeat):(10 * numFeat)]), columns=objInfoNames[:-1])\n",
    "\n",
    "# Recombine normalized sets to one data frame\n",
    "normalizedDataPoolAllLev = pd.concat([normalizedFeat2, normalizedFeat3, normalizedFeatBase, normalizedFeat5,\n",
    "                                      normalizedFeat6, normalizedFeat7, normalizedFeat8, normalizedFeat9,\n",
    "                                      normalizedFeat10, normalizedFeat11, normalizedLabelUSE], axis=1)\n",
    "\n",
    "# Remove used temporary variables\n",
    "del normalizedFeat, normalizedFeat2, normalizedFeat3, normalizedFeatBase, normalizedFeat5, normalizedFeat6\n",
    "del normalizedFeat7, normalizedFeat8, normalizedFeat9, normalizedFeat10, normalizedFeat11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test, train, and validate data\n",
    "trainDataPoolAllLev, testDataAllLev, validateDataAllLev = [df for _, df in normalizedDataPoolAllLev.groupby('USE')]\n",
    "trainDataPoolAllLev = trainDataPoolAllLev.iloc[:, :-1]\n",
    "testDataAllLev = testDataAllLev.iloc[:, :-1]\n",
    "validateFeatAllLev = validateDataAllLev.iloc[:, :-2]\n",
    "validateLabels = validateDataAllLev.iloc[:, -2]\n",
    "\n",
    "# Order train data pool by class label in alphabetical order\n",
    "trainDataPoolAllLev = trainDataPoolAllLev.sort_values(by=trainDataPoolAllLev.columns[-1])\n",
    "\n",
    "# Current training data-set, updated (refreshed) after each iteration\n",
    "trainDataCur = trainDataPoolAllLev.copy()\n",
    "testDataCur = testDataAllLev.copy()\n",
    "\n",
    "# Set randomized seed for the random sampling procedure\n",
    "seed = 5\n",
    "\n",
    "# Initial seed value for randomized sampling\n",
    "seed += np.random.randint(1, 101)\n",
    "\n",
    "# Definition of apriori-probabilities\n",
    "pA = pB = pC = pD = pE = pF = 1 / 6\n",
    "\n",
    "# Definition of training sample set sizes S [% of max. sample size]\n",
    "sCur = sMax * (sampleSizesPor[0] / 100)\n",
    "# Definition of sample shares\n",
    "nA, nB, nC, nD, nE, nF = [round(sCur * p) for p in [pA, pB, pC, pD, pE, pF]]\n",
    "shares = np.array([nA, nB, nC, nD, nE, nF])\n",
    "\n",
    "# Set randomized seed for the random sampling procedure\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validateDataAllLev.columns[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp = trainDataCur.groupby('REF', observed=False)[trainDataCur.columns].apply(lambda x: x.sample(67, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling function\n",
    "def sample_within_group(group):\n",
    "    # Add the original IDs as a new column\n",
    "    group['ID_unit'] = group.index\n",
    "    \n",
    "    # Perform sampling within the group\n",
    "    sampled_group = group.sample(min(len(group), 67), replace=False)\n",
    "    \n",
    "    return sampled_group\n",
    "\n",
    "# Apply the sampling function to each group\n",
    "stratSamp = trainDataCur.groupby('REF', observed=False).apply(sample_within_group)\n",
    "\n",
    "# Reset the index to obtain a flat DataFrame with the original IDs preserved\n",
    "stratSamp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get samples of trainDataCur and set trainDataCur new\n",
    "trainDataCurRemaining = trainDataCur.drop(stratSamp[\"ID_unit\"])\n",
    "\n",
    "# Split test feat from test label for later join with trainData\n",
    "trainFeat = stratSamp.iloc[:, :len(trainDataPoolAllLev.columns)-1]\n",
    "trainLabels = stratSamp.iloc[:, len(trainDataPoolAllLev.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(trainDataPoolAllLev.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainFeat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp.columns[len(trainDataPoolAllLev.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp = trainDataCur.groupby('REF',observed=False)\n",
    "#stratSamp = stratSamp.apply(lambda x: x.sample(67, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of sampling configuration (strata: random sampling without replacement)\n",
    "#stratSamp = trainDataCur.groupby('REF').apply(lambda x: x.sample(shares, replace=False)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDataCur.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp.iloc[:,181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp[\"ID_unit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainFeat.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testDataCur.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testFeatsub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "class CustomSplitter(BaseCrossValidator):\n",
    "    def __init__(self, index_train):\n",
    "        self.index_train = index_train\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        test_indices = np.setdiff1d(np.arange(len(X)), self.index_train)\n",
    "        yield self.index_train, test_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subset for each outer iteration test data to speed up computing\n",
    "testDataCur = testDataCur.sort_values(by=testDataCur.columns[-1])\n",
    "# Apply the sampling function to each group\n",
    "stratSamp = testDataCur.groupby('REF', observed=False).apply(sample_within_group)\n",
    "\n",
    "# Split test feat from test label for later join with trainData\n",
    "testFeat = stratSamp.iloc[:, :len(testDataCur.columns)-1]\n",
    "testLabels = stratSamp.iloc[:, len(testDataCur.columns)-1]\n",
    "\n",
    "# Subset on base level\n",
    "testFeatsub = testFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# TrainData index to split between train and test in svmFit\n",
    "countTrainData = trainFeat.shape[0]\n",
    "indexTrainData = [list(range(countTrainData))]\n",
    "\n",
    "# SVM base for invariants\n",
    "\n",
    "# Subset on L_4\n",
    "trainFeat = trainFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# Join train and test data (separable through indexTrainData in svmFit)\n",
    "tuneFeat = pd.concat([trainFeat, testFeatsub], axis=0)\n",
    "tuneLabel = np.concatenate((trainLabels.values, testLabels.values))\n",
    "\n",
    "validateFeatsub = validateFeatAllLev.iloc[:, sindexSVMDATA:eindexSVMDATA+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexTrainData =np.array(range(countTrainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuneFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "       147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,\n",
       "       160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
       "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
       "       186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
       "       199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
       "       264, 265, 266, 267])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(np.arange(len(tuneFeat)), indexTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_splitter = CustomSplitter(indexTrainData,tuneFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSplitter(BaseCrossValidator):\n",
    "    def __init__(self, index_train):\n",
    "        self.index_train = index_train\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        test_indices = np.setdiff1d(np.arange(len(X)), self.index_train)\n",
    "        yield self.index_train, test_indices\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return 1  # Since you're doing a single split, return 1\n",
    "\n",
    "countTrainData = trainFeat.shape[0]\n",
    "indexTrainData = np.array(range(countTrainData))\n",
    "\n",
    "# SVM base for invariants\n",
    "\n",
    "# Subset on L_4\n",
    "trainFeat = trainFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# Join train and test data (separable through indexTrainData in svmFit)\n",
    "tuneFeat = pd.concat([trainFeat, testFeatsub], axis=0)\n",
    "tuneLabel = np.concatenate((trainLabels.values, testLabels.values))\n",
    "\n",
    "custom_splitter = CustomSplitter(indexTrainData)\n",
    "\n",
    "# SVM parameter tuning\n",
    "svm_model=svm_fit(tuneFeat, tuneLabel, custom_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 18)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuneFeat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM parameter tuning\n",
    "svm_model=svm_fit(tuneFeat, tuneLabel, custom_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bushes_trees    67\n",
       "other           67\n",
       "Name: REF, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels.unique()\n",
    "trainLabels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8768101811156146\n"
     ]
    }
   ],
   "source": [
    "# Predict labels of test data\n",
    "predLabelsSVM = svm_model.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accSVM = accuracy_score(validateLabels, predLabelsSVM)\n",
    "print(accSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainDataCur.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other           357365\n",
       "bushes_trees     48420\n",
       "Name: REF, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataCur['REF'].unique()\n",
    "trainDataCur['REF'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bushes_trees', 'other'], dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11770877, 0.263158  , 0.61822079, 0.44276951, 0.14035688,\n",
       "        0.02920971, 0.0208755 , 0.55691075, 0.06458741, 0.0222087 ,\n",
       "        0.06751957, 0.05327685, 0.17553671, 0.07652946, 0.05125174,\n",
       "        0.18771871, 0.03578111, 0.79301248],\n",
       "       [0.08841473, 0.675841  , 0.82627553, 0.21814325, 0.13382892,\n",
       "        0.02696745, 0.01792254, 0.4641897 , 0.04431946, 0.02005523,\n",
       "        0.03643325, 0.02949871, 0.30568572, 0.08655693, 0.02822726,\n",
       "        0.12758109, 0.04270372, 0.81109333],\n",
       "       [0.04809533, 0.861538  , 0.89016515, 0.08527873, 0.12648156,\n",
       "        0.03207706, 0.02740113, 0.51226972, 0.06290198, 0.02909166,\n",
       "        0.04648648, 0.04678275, 0.59508436, 0.15669978, 0.05465299,\n",
       "        0.10566308, 0.06175637, 0.79804385],\n",
       "       [0.20875686, 0.        , 0.41692503, 0.47196826, 0.29781484,\n",
       "        0.02770935, 0.02053573, 0.51643706, 0.05501459, 0.02121302,\n",
       "        0.03990251, 0.03671958, 0.24703117, 0.08141916, 0.02936107,\n",
       "        0.17569506, 0.02934755, 0.80734324]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.best_estimator_.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   9,  14,  15,  18,  19,  23,  27,  28,  29,  30,  38,  39,\n",
       "        51,  59,  60,  64,  68,  72,  73,  75,  78,  84,  97,  99, 103,\n",
       "       111, 112, 114, 115, 117, 119, 121, 129, 131])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "      <th>REF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141566</th>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.805668</td>\n",
       "      <td>0.873817</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>0.092888</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.032189</td>\n",
       "      <td>0.620288</td>\n",
       "      <td>0.100116</td>\n",
       "      <td>0.036525</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.405775</td>\n",
       "      <td>0.165309</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.090419</td>\n",
       "      <td>0.052963</td>\n",
       "      <td>0.796689</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35439</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101611</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "      <td>0.803718</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101610</th>\n",
       "      <td>0.052193</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.142156</td>\n",
       "      <td>0.049890</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>0.678320</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>0.091693</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.213381</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.770114</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl  \\\n",
       "141566   0.068239   0.805668   0.873817   0.141781   0.092888  0.033785   \n",
       "35439    0.094363   0.594595   0.789575   0.273995   0.186283  0.029562   \n",
       "101611   0.073022   0.670330   0.798266   0.201985   0.077653  0.021294   \n",
       "101610   0.052193   0.707317   0.819672   0.142156   0.049890  0.033915   \n",
       "\n",
       "         Lx_m_gr  Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  \\\n",
       "141566  0.032189   0.620288  0.100116  0.036525  0.040698  0.037860   \n",
       "35439   0.038956   0.658483  0.126350  0.045296  0.087441  0.108569   \n",
       "101611  0.014574   0.631935  0.061670  0.007359  0.026509  0.022434   \n",
       "101610  0.035445   0.678320  0.109000  0.029649  0.026247  0.020902   \n",
       "\n",
       "        Lx_sd_ndvi  Lx_sd_nir  Lx_sd_re  Lx_t_diss  Lx_t_hom  Lx_t_mean  \\\n",
       "141566    0.405775   0.165309  0.044026   0.090419  0.052963   0.796689   \n",
       "35439     0.624496   0.336826  0.159947   0.146974  0.050292   0.826420   \n",
       "101611    0.316412   0.119451  0.020073   0.098026  0.060738   0.803718   \n",
       "101610    0.091693   0.051384  0.018004   0.213381  0.016879   0.770114   \n",
       "\n",
       "                 REF  \n",
       "141566  bushes_trees  \n",
       "35439   bushes_trees  \n",
       "101611  bushes_trees  \n",
       "101610  bushes_trees  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataCur.iloc[SVindex, list(range(sindexSVMDATA,eindexSVMDATA+1))+[-1]][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>...</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "      <th>REF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109760</th>\n",
       "      <td>0.171857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.713605</td>\n",
       "      <td>0.238544</td>\n",
       "      <td>0.152226</td>\n",
       "      <td>0.034789</td>\n",
       "      <td>0.036164</td>\n",
       "      <td>0.453214</td>\n",
       "      <td>0.061155</td>\n",
       "      <td>0.049238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139337</td>\n",
       "      <td>0.038537</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.044961</td>\n",
       "      <td>0.017197</td>\n",
       "      <td>0.033769</td>\n",
       "      <td>0.648476</td>\n",
       "      <td>0.135342</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141565</th>\n",
       "      <td>0.081891</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.838557</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.097356</td>\n",
       "      <td>0.033760</td>\n",
       "      <td>0.037295</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.123907</td>\n",
       "      <td>0.044875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119707</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>0.016966</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>0.689078</td>\n",
       "      <td>0.203255</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141566</th>\n",
       "      <td>0.081891</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.838557</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.097356</td>\n",
       "      <td>0.033760</td>\n",
       "      <td>0.037295</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.123907</td>\n",
       "      <td>0.044875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119707</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>0.016966</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>0.689078</td>\n",
       "      <td>0.203255</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101618</th>\n",
       "      <td>0.292604</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.477876</td>\n",
       "      <td>0.597481</td>\n",
       "      <td>0.375123</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.031084</td>\n",
       "      <td>0.709437</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.031764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119707</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>0.016966</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>0.689078</td>\n",
       "      <td>0.203255</td>\n",
       "      <td>bushes_trees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl  \\\n",
       "109760   0.171857   0.666667   0.713605   0.238544   0.152226  0.034789   \n",
       "141565   0.081891   0.650000   0.838557   0.131868   0.097356  0.033760   \n",
       "141566   0.081891   0.650000   0.838557   0.131868   0.097356  0.033760   \n",
       "101618   0.292604   0.037037   0.477876   0.597481   0.375123  0.030327   \n",
       "\n",
       "         Lx_m_gr  Lx_m_ndvi  Lx_m_nir   Lx_m_re  ...   Lx_m_re  Lx_sd_bl  \\\n",
       "109760  0.036164   0.453214  0.061155  0.049238  ...  0.139337  0.038537   \n",
       "141565  0.037295   0.722255  0.123907  0.044875  ...  0.119707  0.023619   \n",
       "141566  0.037295   0.722255  0.123907  0.044875  ...  0.119707  0.023619   \n",
       "101618  0.031084   0.709437  0.096326  0.031764  ...  0.119707  0.023619   \n",
       "\n",
       "        Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  Lx_sd_re  Lx_t_diss  Lx_t_hom  \\\n",
       "109760  0.026021    0.000079   0.044961  0.017197   0.033769  0.648476   \n",
       "141565  0.016966    0.000091   0.068199  0.015982   0.028416  0.689078   \n",
       "141566  0.016966    0.000091   0.068199  0.015982   0.028416  0.689078   \n",
       "101618  0.016966    0.000091   0.068199  0.015982   0.028416  0.689078   \n",
       "\n",
       "        Lx_t_mean           REF  \n",
       "109760   0.135342  bushes_trees  \n",
       "141565   0.203255  bushes_trees  \n",
       "141566   0.203255  bushes_trees  \n",
       "101618   0.203255  bushes_trees  \n",
       "\n",
       "[4 rows x 181 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataCur[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.best_estimator_.support_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 19)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVtotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bushes_trees    217\n",
       "other             0\n",
       "Name: REF, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataCur.iloc[svm_model.best_estimator_.support_, -1].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   6,   8,   9,  10,  11,  12,  13,  14,  15,\n",
       "        16,  17,  18,  19,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "        31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  42,  43,  44,\n",
       "        45,  46,  47,  48,  49,  50,  52,  53,  54,  55,  58,  59,  60,\n",
       "        62,  63,  65,  66, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 155, 156, 157,\n",
       "       158, 159, 160, 161, 163, 164, 165, 166, 168, 169, 170, 171, 172,\n",
       "       174, 176, 177, 178, 180, 181, 182, 189, 190, 192, 193, 194, 195,\n",
       "       196, 197, 199, 200,  67,  68,  70,  71,  72,  73,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  85,  86,  87,  89,  90,  91,  92,\n",
       "        93,  94,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 108,\n",
       "       109, 112, 113, 118, 119, 120, 121, 123, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 201, 202, 204, 205, 206, 208, 209, 210, 211,\n",
       "       212, 213, 214, 215, 217, 218, 219, 221, 222, 223, 224, 225, 226,\n",
       "       227, 228, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "       242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255,\n",
       "       256, 258, 260, 261, 263, 264, 265, 266, 267])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.best_estimator_.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSVM on all Level SV\n",
    "SVindex = svm_model.best_estimator_.support_  # Indices of support vectors\n",
    "SVtotal = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA,eindexSVMDATA+1))+[-1]].reset_index(drop=True)  # Get support vectors\n",
    "\n",
    "SVL2 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA - 2 * numFeat,sindexSVMDATA - numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL3 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA - numFeat,sindexSVMDATA))+[-1] ].reset_index(drop=True)\n",
    "\n",
    "SVL5 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + numFeat,sindexSVMDATA + 2 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL6 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + 2 * numFeat,sindexSVMDATA + 3 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL7 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + 3 * numFeat,sindexSVMDATA + 4 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL8 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + 4 * numFeat,sindexSVMDATA + 5 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL9 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + 5 * numFeat,sindexSVMDATA + 6 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL10 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + 6 * numFeat,sindexSVMDATA + 7 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "SVL11 = trainDataCur.iloc[SVindex, list(range(sindexSVMDATA + 7 * numFeat,sindexSVMDATA + 8 * numFeat))+[-1] ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Bind original SV with modified to new train data set\n",
    "SVinvar = pd.concat([SVtotal, SVL2, SVL3, SVL5, SVL6, SVL7, SVL8, SVL9, SVL10, SVL11],ignore_index=True)\n",
    "\n",
    "# Split for training to feature and label\n",
    "trainFeatVSVM = SVinvar.iloc[:, :-1].reset_index(drop=True)\n",
    "trainLabelsVSVM = SVinvar.iloc[:, -1]\n",
    "\n",
    "# Get list with index of train data to split between train and test in svmFit\n",
    "countTrainData = SVinvar.shape[0]\n",
    "indexTrainData = [list(range(1, countTrainData + 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainLabelsVSVM.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainFeatVSVM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuneFeatVSVM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuneFeatVSVM.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8049789543475848\n"
     ]
    }
   ],
   "source": [
    "# Join of train and test test data (through indexTrainData in svmFit separable)\n",
    "tuneFeatVSVM = pd.concat([trainFeatVSVM, testFeatsub])\n",
    "tuneLabelsVSVM = np.concatenate((trainLabelsVSVM.values, testLabels.values))\n",
    "\n",
    "# VSVM parameter tuning\n",
    "tunedVSVM = svm_fit(tuneFeatVSVM, tuneLabelsVSVM)\n",
    "\n",
    "# Run classification and accuracy assessment for modified SV\n",
    "predLabelsVSVM = tunedVSVM.predict(validateFeatsub)\n",
    "accVSVM = accuracy_score(validateLabels, predLabelsVSVM)\n",
    "print(accVSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.merge(trainFeatVSVM, testFeatsub, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainFeatVSVM[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testFeatsub[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([SVtotal,SVL2,SVL5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Iteration over bound to test different bound thresholds determining the radius of acceptance\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m jj \u001b[38;5;129;01min\u001b[39;00m bound:\n\u001b[0;32m      7\u001b[0m     SVinvarRadi \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mrem_extrem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSVtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSVL2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      9\u001b[0m     rem_extrem(SVtotal, SVL3, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     10\u001b[0m     rem_extrem(SVtotal, SVL5, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     11\u001b[0m     rem_extrem(SVtotal, SVL6, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     12\u001b[0m     rem_extrem(SVtotal, SVL7, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     13\u001b[0m     rem_extrem(SVtotal, SVL8, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     14\u001b[0m     rem_extrem(SVtotal, SVL9, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     15\u001b[0m     rem_extrem(SVtotal, SVL10, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     16\u001b[0m     rem_extrem(SVtotal, SVL11, jj)\u001b[38;5;241m.\u001b[39mset_axis(objInfoNames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# rem_extrem(SVtotal, SVL12, bound[jj]).set_axis(objInfoNames, axis=1),\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# rem_extrem(SVtotal, SVL13, bound[jj]).set_axis(objInfoNames, axis=1)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     ])    \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Iterating over boundMargin to test different thresholds on margin distance\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kk, bound_margin_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(boundMargin):\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mrem_extrem\u001b[1;34m(org, VSV1, a)\u001b[0m\n\u001b[0;32m     16\u001b[0m     distance\u001b[38;5;241m.\u001b[39mloc[l, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m euc_dis(org\u001b[38;5;241m.\u001b[39miloc[l, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], VSV1\u001b[38;5;241m.\u001b[39miloc[l, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     18\u001b[0m SVClass1 \u001b[38;5;241m=\u001b[39m org[org[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m org[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREF\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m---> 19\u001b[0m SVClass2 \u001b[38;5;241m=\u001b[39m org[org[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43morg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(SVClass1) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(SVClass1) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\advpy\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:289\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDArrayBackedExtensionArrayT,\n\u001b[0;32m    285\u001b[0m     key: PositionalIndexer2D,\n\u001b[0;32m    286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDArrayBackedExtensionArrayT \u001b[38;5;241m|\u001b[39m Any:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_integer(key):\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;66;03m# fast-path\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    291\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_box_func(result)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# VSVM - EVALUATION of all Level VSV\n",
    "actKappa = 0\n",
    "bestFittingModel = None\n",
    "\n",
    "# Iteration over bound to test different bound thresholds determining the radius of acceptance\n",
    "for jj in bound:\n",
    "    SVinvarRadi = pd.concat([\n",
    "    rem_extrem(SVtotal, SVL2, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL3, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL5, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL6, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL7, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL8, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL9, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL10, jj).set_axis(objInfoNames, axis=1),\n",
    "    rem_extrem(SVtotal, SVL11, jj).set_axis(objInfoNames, axis=1),\n",
    "    # rem_extrem(SVtotal, SVL12, bound[jj]).set_axis(objInfoNames, axis=1),\n",
    "    # rem_extrem(SVtotal, SVL13, bound[jj]).set_axis(objInfoNames, axis=1)\n",
    "    ])    \n",
    "    # Iterating over boundMargin to test different thresholds on margin distance\n",
    "    for kk, bound_margin_val in enumerate(boundMargin):\n",
    "        SVinvar_list = []\n",
    "        \n",
    "        # Iterate over SVinvarRadi and evaluate distance to hyperplane\n",
    "        for m in range(len(SVinvarRadi)):\n",
    "            signa = pred_one(tunedVSVM, SVinvarRadi[m, :-1])\n",
    "            if SVinvarRadi[m, -1] == levels(generalDataPool.REF)[0]:\n",
    "                if -bound_margin_val < signa < bound_margin_val:\n",
    "                    SVinvar_list.append(SVinvarRadi[m, :])\n",
    "            else:\n",
    "                if -bound_margin_val < signa < bound_margin_val:\n",
    "                    SVinvar_list.append(SVinvarRadi[m, :])\n",
    "\n",
    "        SVinvar = pd.DataFrame(SVinvar_list, columns=objInfoNames)\n",
    "        \n",
    "        # Merge elected VSV with original SV\n",
    "        SVinvar_org = pd.concat([SVtotal, SVinvar])\n",
    "\n",
    "        # Split for training to feature and label\n",
    "        trainFeatVSVM = SVinvar_org.iloc[:, :-1]\n",
    "        trainLabelsVSVM = SVinvar_org.iloc[:, -1]\n",
    "\n",
    "        # Get list with index of trainData to split between train and test in svmFit\n",
    "        countTrainData = SVinvar_org.shape[0]\n",
    "        indexTrainData = [list(range(1, countTrainData + 1))]\n",
    "\n",
    "        # Join of train and test data (through indexTrainData in svmFit separable)\n",
    "        names = objInfoNames[:-1]\n",
    "        tuneFeatVSVM = pd.concat([trainFeatVSVM, testFeatsub], axis=0)\n",
    "        tuneFeatVSVM.columns = names\n",
    "        tuneLabelsVSVM = np.concatenate((trainLabelsVSVM.values, testLabels.values))\n",
    "\n",
    "        ######################################## VSVM control parameter tuning ########################################\n",
    "        tunedVSVM = SVC(kernel='linear')\n",
    "        tunedVSVM.fit(tuneFeatVSVM, tuneLabelsVSVM)\n",
    "\n",
    "        # Get the best fitting model based on Kappa\n",
    "        if actKappa < tunedVSVM.resample.Kappa:\n",
    "            bestFittingModel = tunedVSVM\n",
    "            actKappa = tunedVSVM.resample.Kappa\n",
    "\n",
    "# Run classification and accuracy assessment for the best bound setting\n",
    "# Predict labels of test data\n",
    "predLabelsVSVMsum = bestFittingModel.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL = accuracy_score(validateLabels, predLabelsVSVMsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Balanced & Random unlabeled samples\n",
    "# Balanced samples\n",
    "\n",
    "# Definition of sampling configuration (strata: random sampling without replacement)\n",
    "stratSampRemaining_b = resample(trainDataCurRemaining, n_samples=[b, b, b, b, b, b], replace=False)\n",
    "samplesRemaining_b = trainDataCurRemaining.iloc[stratSampRemaining_b]\n",
    "\n",
    "trainDataCurRemaining_b = samplesRemaining_b.iloc[:, :-1]\n",
    "trainDataCurRemainingsub_b = trainDataCurRemaining_b.iloc[:, sindexSVMDATA:eindexSVMDATA]\n",
    "REF_b = bestFittingModel.predict(trainDataCurRemainingsub_b)\n",
    "\n",
    "SVindexUn_b = np.arange(1, len(trainDataCurRemainingsub_b) + 1)\n",
    "SVtotalUn_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA:eindexSVMDATA]\n",
    "SVtotalUn_b['REF'] = REF_b\n",
    "\n",
    "SVL2Un_b = trainDataCurRemaining.iloc[SVindexUn_b - 1, sindexSVMDATA - 2*numFeat:sindexSVMDATA - numFeat - 1].copy()\n",
    "SVL2Un_b['REF'] = REF_b\n",
    "SVL3Un_b = trainDataCurRemaining.iloc[SVindexUn_b - 1, sindexSVMDATA - numFeat:sindexSVMDATA - 1].copy()\n",
    "SVL3Un_b['REF'] = REF_b\n",
    "SVL5Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + numFeat:sindexSVMDATA + 2*numFeat - 1].copy()\n",
    "SVL5Un_b['REF'] = REF_b\n",
    "SVL6Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 2*numFeat:sindexSVMDATA + 3*numFeat - 1].copy()\n",
    "SVL6Un_b['REF'] = REF_b\n",
    "SVL7Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 3*numFeat:sindexSVMDATA + 4*numFeat - 1].copy()\n",
    "SVL7Un_b['REF'] = REF_b\n",
    "SVL8Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 4*numFeat:sindexSVMDATA + 5*numFeat - 1].copy()\n",
    "SVL8Un_b['REF'] = REF_b\n",
    "SVL9Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 5*numFeat:sindexSVMDATA + 6*numFeat - 1].copy()\n",
    "SVL9Un_b['REF'] = REF_b\n",
    "SVL10Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 6*numFeat:sindexSVMDATA + 7*numFeat - 1].copy()\n",
    "SVL10Un_b['REF'] = REF_b\n",
    "SVL11Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 7*numFeat:sindexSVMDATA + 8*numFeat - 1].copy()\n",
    "SVL11Un_b['REF'] = REF_b\n",
    "\n",
    "SVinvarUn_b = pd.concat([SVtotalUn_b, SVL2Un_b, SVL3Un_b, SVL5Un_b, SVL6Un_b, SVL7Un_b, SVL8Un_b, SVL9Un_b, SVL10Un_b, SVL11Un_b])\n",
    "\n",
    "# Balanced Unlabeled samples\n",
    "\n",
    "actKappa = 0\n",
    "\n",
    "for jj in range(len(bound)):\n",
    "    SVinvarRadiUn_b_list = []\n",
    "    for m in range(len(SVinvarRadiUn_b)):\n",
    "        signa = pred_one(tunedSVM.finalModel, SVinvarRadiUn_b.iloc[m, :-1])\n",
    "        if SVinvarRadiUn_b.iloc[m, -1] == levels(generalDataPool.REF)[0]:\n",
    "            if -bound_margin_val < signa < bound_margin_val:\n",
    "                SVinvarRadiUn_b_list.append(SVinvarRadiUn_b.iloc[m, :])\n",
    "        else:\n",
    "            if -bound_margin_val < signa < bound_margin_val:\n",
    "                SVinvarRadiUn_b_list.append(SVinvarRadiUn_b.iloc[m, :])\n",
    "\n",
    "    SVinvarUn_b = pd.DataFrame(SVinvarRadiUn_b_list, columns=objInfoNames)\n",
    "    \n",
    "    SVinvar_orgUn_b = pd.concat([SVtotal, SVinvarUn_b])\n",
    "\n",
    "    trainFeatVSVMUn_b = SVinvar_orgUn_b.iloc[:, :-1]\n",
    "    trainLabelsVSVMUn_b = SVinvar_orgUn_b.iloc[:, -1]\n",
    "\n",
    "    countTrainDataUn_b = SVinvar_orgUn_b.shape[0]\n",
    "    indexTrainDataUn_b = [list(range(1, countTrainDataUn_b + 1))]\n",
    "\n",
    "    names = objInfoNames[:-1]\n",
    "    tuneFeatVSVMUn_b = pd.concat([trainFeatVSVMUn_b, testFeatsub], axis=0)\n",
    "    tuneFeatVSVMUn_b.columns = names\n",
    "    tuneLabelsVSVMUn_b = np.concatenate((trainLabelsVSVMUn_b.values, testLabels.values))\n",
    "\n",
    "    tunedVSVMUn_b = SVC(kernel='linear')\n",
    "    tunedVSVMUn_b.fit(tuneFeatVSVMUn_b, tuneLabelsVSVMUn_b)\n",
    "\n",
    "    if actKappa < tunedVSVMUn_b.resample.Kappa:\n",
    "        bestFittingModelUn_b = tunedVSVMUn_b\n",
    "        actKappa = tunedVSVMUn_b.resample.Kappa\n",
    "\n",
    "# Run classification and accuracy assessment for the best bound setting\n",
    "predLabelsVSVMsumUn_b = bestFittingModelUn_b.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL_Un_b = accuracy_score(validateLabels, predLabelsVSVMsumUn_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted labels to the features data set\n",
    "predLabelsVSVMsumUn_unc = pd.concat([validateFeatsub, pd.DataFrame(predLabelsVSVMsumUn_b, columns=[\"Predicted_Labels\"])], axis=1)\n",
    "predLabelsVSVMsumUn_unc.columns = objInfoNames\n",
    "\n",
    "# Calculate uncertainty of the samples by selecting SV's and data set\n",
    "normdistvsvm_sl_un = uncertainty_dist_v2_2(bestFittingModelUn_b, predLabelsVSVMsumUn_unc)\n",
    "\n",
    "# Alter labels\n",
    "predlabels_vsvm_Slu = alter_labels(normdistvsvm_sl_un, validateLabels)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL_Un_b_ad = accuracy_score(validateLabels, predlabels_vsvm_Slu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

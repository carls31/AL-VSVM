{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#from dfply import arrange\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path1 = \"D:/tunc_oz/apply_model\"\n",
    "path1 = \"/home/rsrg9/Documents/tunc_oz/apply_model\"\n",
    "os.chdir(path1)\n",
    "\n",
    "# Second path\n",
    "path2 = \"csv_data_r_import/cologne/scale\"\n",
    "os.chdir(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(x, y, index_train=None):\n",
    "    # Expand coarse grid\n",
    "    coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                   'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "    \n",
    "    kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "    # Coarse grid search\n",
    "    svm_coarse = SVC(kernel='rbf')\n",
    "    svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring=kappa_scorer)#, cv=index_train)\n",
    "    svm_coarse_cv.fit(x, y)\n",
    "    \n",
    "    # Get best coarse grid parameters\n",
    "    best_c = svm_coarse_cv.best_params_['C']\n",
    "    best_gamma = svm_coarse_cv.best_params_['gamma']\n",
    "    \n",
    "    # Define narrow grid borders\n",
    "    a_gamma = np.log2(best_gamma) - 2\n",
    "    b_gamma = np.log2(best_gamma) + 2\n",
    "    a_c = np.log2(best_c) - 2\n",
    "    b_c = np.log2(best_c) + 2\n",
    "    \n",
    "    # Expand narrow grid\n",
    "    narrow_grid = {'C': 2.0 ** np.arange(a_c, b_c, 0.5),\n",
    "                   'gamma': 2.0 ** np.arange(a_gamma, b_gamma, 0.5)}\n",
    "    \n",
    "    # Narrow grid search\n",
    "    svm_narrow = SVC(kernel='rbf')\n",
    "    svm_narrow_cv = GridSearchCV(svm_narrow, param_grid=narrow_grid, scoring=kappa_scorer)#, cv=index_train)\n",
    "    svm_narrow_cv.fit(x, y)\n",
    "    \n",
    "    return svm_narrow_cv\n",
    "\n",
    "# Usage example:\n",
    "# svm_model = svm_fit(X_train, y_train, StratifiedKFold(n_splits=10, shuffle=True))\n",
    "\n",
    "# Euclidean Distance between two points lying in the input space\n",
    "def euc_dis(a, b):\n",
    "    temp = 0\n",
    "    for ii in range(len(a)):\n",
    "        temp += (a[ii] - b[ii])**2\n",
    "    return np.sqrt(temp)\n",
    "\n",
    "# Evaluate the distance between Virtual Support Vectors and Support Vectors lying in the input space\n",
    "def rem_extrem(org, VSV1, a):\n",
    "    distance = pd.DataFrame(index=range(len(org)), columns=['label', 'distance'])\n",
    "    distanceSVC1 = []\n",
    "    distanceSVC2 = []\n",
    "    \n",
    "    for l in range(len(org)):\n",
    "        distance.loc[l, 'label'] = str(org.iloc[l, -1])\n",
    "        distance.loc[l, 'distance'] = euc_dis(org.iloc[l, :-1], VSV1.iloc[l, :-1])\n",
    "    \n",
    "    SVClass1 = org[org['REF'] == org['REF'].unique()[0]]\n",
    "    SVClass2 = org[org['REF'] == org['REF'].unique()[1]]\n",
    "    \n",
    "    if len(SVClass1) > 0:\n",
    "        for n in range(len(SVClass1) - 1):\n",
    "            for nn in range(n, len(SVClass1) - 1):\n",
    "                distanceSVC1.append(euc_dis(SVClass1.iloc[n, :-1], SVClass1.iloc[n + nn, :-1]))\n",
    "        disClass1median = np.mean(distanceSVC1)\n",
    "        boundClass1 = disClass1median * a\n",
    "    \n",
    "    if len(SVClass2) > 0:\n",
    "        for n in range(len(SVClass2) - 1):\n",
    "            for nn in range(n, len(SVClass2) - 1):\n",
    "                distanceSVC2.append(euc_dis(SVClass2.iloc[n, :-1], SVClass2.iloc[n + nn, :-1]))\n",
    "        disClass2median = np.mean(distanceSVC2)\n",
    "        boundClass2 = disClass2median * a\n",
    "    \n",
    "    for k in range(len(org)):\n",
    "        if np.isnan(distance.loc[k, 'distance']):\n",
    "            VSV1.iloc[k, :] = np.nan\n",
    "        else:\n",
    "            if boundClass1 is not None:\n",
    "                if distance.loc[k, 'label'] == org['REF'].unique()[0]:\n",
    "                    if distance.loc[k, 'distance'] > boundClass1:\n",
    "                        VSV1.iloc[k, :] = np.nan\n",
    "            else:\n",
    "                if boundClass2 is not None:\n",
    "                    if distance.loc[k, 'label'] == org['REF'].unique()[1]:\n",
    "                        if distance.loc[k, 'distance'] > boundClass2:\n",
    "                            VSV1.iloc[k, :] = np.nan\n",
    "    return VSV1\n",
    "\n",
    "# Kernel distance between two points lying in the hyperspace\n",
    "def kern_dis(a, b, kernelfunc):\n",
    "    a = np.array(a).flatten()\n",
    "    b = np.array(b).flatten()\n",
    "    dk = np.sqrt(kernelfunc(a, a) + kernelfunc(b, b) - 2 * kernelfunc(a, b))\n",
    "    return dk\n",
    "\n",
    "# Evaluate the distance between Virtual Support Vectors and Support Vectors lying in the hyperspace\n",
    "def rem_extrem_kerneldist(org, VSV1, a, kernelfunc):\n",
    "    distance = pd.DataFrame(index=range(len(org)), columns=['label', 'distance'])\n",
    "    distanceSVC1 = []\n",
    "    distanceSVC2 = []\n",
    "    \n",
    "    for l in range(len(org)):\n",
    "        distance.loc[l, 'label'] = str(org.iloc[l, -1])\n",
    "        distance.loc[l, 'distance'] = kern_dis(org.iloc[l, :-1], VSV1.iloc[l, :-1], kernelfunc)\n",
    "    \n",
    "    SVClass1 = org[org['REF'] == org['REF'].unique()[0]]\n",
    "    SVClass2 = org[org['REF'] == org['REF'].unique()[1]]\n",
    "    \n",
    "    if len(SVClass1) > 0:\n",
    "        for n in range(len(SVClass1) - 1):\n",
    "            for nn in range(n, len(SVClass1) - 1):\n",
    "                distanceSVC1.append(kern_dis(SVClass1.iloc[n, :-1], SVClass1.iloc[n + nn, :-1], kernelfunc))\n",
    "        disClass1median = np.mean(distanceSVC1)\n",
    "        boundClass1 = disClass1median * a\n",
    "    \n",
    "    if len(SVClass2) > 0:\n",
    "        for n in range(len(SVClass2) - 1):\n",
    "            for nn in range(n, len(SVClass2) - 1):\n",
    "                distanceSVC2.append(kern_dis(SVClass2.iloc[n, :-1], SVClass2.iloc[n + nn, :-1], kernelfunc))\n",
    "        disClass2median = np.mean(distanceSVC2)\n",
    "        boundClass2 = disClass2median * a\n",
    "    \n",
    "    for k in range(len(org)):\n",
    "        if np.isnan(distance.loc[k, 'distance']):\n",
    "            VSV1.iloc[k, :] = np.nan\n",
    "        else:\n",
    "            if boundClass1 is not None:\n",
    "                if distance.loc[k, 'label'] == org['REF'].unique()[0]:\n",
    "                    if distance.loc[k, 'distance'] > boundClass1:\n",
    "                        VSV1.iloc[k, :] = np.nan\n",
    "            else:\n",
    "                if boundClass2 is not None:\n",
    "                    if distance.loc[k, 'label'] == org['REF'].unique()[1]:\n",
    "                        if distance.loc[k, 'distance'] > boundClass2:\n",
    "                            VSV1.iloc[k, :] = np.nan\n",
    "    return VSV1\n",
    "\n",
    "def pred_one(model, data_point):\n",
    "    # Extract necessary components from the SVM model\n",
    "    support_vectors = model.n_support_\n",
    "    kernel_function = model.kernel\n",
    "    coefficients = model.dual_coef_.ravel()\n",
    "    intercept = model.intercept_\n",
    "    \n",
    "    # Initialize prediction variable\n",
    "    prediction = 0\n",
    "    \n",
    "    # Iterate over each support vector\n",
    "    for j in range(len(support_vectors)):\n",
    "        # Compute kernel function value between the j-th support vector and the data point\n",
    "        kernel_value = kernel_function(data_point.reshape(1, -1), model.support_vectors_[j, :].reshape(1, -1))\n",
    "        \n",
    "        # Multiply kernel value by the corresponding coefficient and add to prediction\n",
    "        weighted_value = kernel_value * coefficients[j]\n",
    "        prediction += weighted_value\n",
    "    \n",
    "    # Subtract intercept to get the final prediction\n",
    "    final_prediction = prediction - intercept\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "def uncertainty_dist_v2_2(org, samp):\n",
    "    distance = pd.DataFrame(columns=['control_label', 'distance'], index=range(len(samp)))\n",
    "    \n",
    "    for k in range(len(samp)):\n",
    "        distance.loc[k, 'distance'] = np.sign(pred_one(org.finalModel, samp.iloc[k, :-1])) * \\\n",
    "                                      np.where(pred_one(org.finalModel, samp.iloc[k, :-1]) > 0, 1, -1)\n",
    "    \n",
    "    # Normalize distance\n",
    "    preProc = preprocessing.MinMaxScaler()\n",
    "    preProc.fit(distance[['distance']])\n",
    "    normdistance = preProc.transform(distance[['distance']])\n",
    "    \n",
    "    samp['normdistance'] = normdistance\n",
    "    \n",
    "    return samp\n",
    "\n",
    "def alter_labels(distance_data, ref):\n",
    "    # Merge features and original labels\n",
    "    ref_added = pd.concat([distance_data, ref], axis=1)\n",
    "    # Order by most uncertain samples\n",
    "    ref_added_or = ref_added.sort_values(by='distance')\n",
    "    # Re-label most uncertain n number of samples\n",
    "    ref_added_or.iloc[:250, -1] = ref_added_or.iloc[:250, -2]\n",
    "    ref_added_or.iloc[:250, -2] = 1.0\n",
    "    # Re-order dataset by its index\n",
    "    ref_added_or['index'] = range(len(ref_added_or))\n",
    "    ref_added_reor = ref_added_or.sort_values(by='index')\n",
    "    \n",
    "    # Extract labels for prediction\n",
    "    labels = ref_added_reor.iloc[:, -5]\n",
    "    return labels\n",
    "\n",
    "def ExCsvMSD(datadase, filename=None):\n",
    "    # Convert to numpy array\n",
    "    datadase = np.array(datadase)\n",
    "    n = datadase.shape[1]\n",
    "    MSDdata = np.empty((2, n), dtype=float)\n",
    "    \n",
    "    MSDdata[0, :] = np.mean(datadase, axis=0)\n",
    "    MSDdata[1, :] = np.std(datadase, axis=0)\n",
    "    \n",
    "    MSDdata_final = np.vstack((datadase, MSDdata))\n",
    "    \n",
    "    # Export final mean and standard deviation to .csv-file\n",
    "    if filename is not None:\n",
    "        pd.DataFrame(MSDdata_final).to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    return MSDdata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"cologne_res_100_L2-L13.csv\"\n",
    "sMax = 1000\n",
    "bound = [0.3, 0.6, 0.9]\n",
    "boundMargin = [1.5, 1.0, 0.5]\n",
    "sampleSizesPor = [40, 25, 16, 12, 10, 8, 6, 4, 3, 2, 1]\n",
    "colheader = [\"40\", \"25\", \"16\", \"12\", \"10\", \"8\", \"6\", \"4\", \"3\", \"2\", \"1\"]\n",
    "sindexSVMDATA = 36\n",
    "numFeat = 18\n",
    "eindexSVMDATA = sindexSVMDATA + numFeat - 1\n",
    "objInfoNames = [\"Lx_g_comp\", \"Lx_g_elfi\", \"Lx_g_refi\", \"Lx_g_roun\", \"Lx_g_shin\",\n",
    "                \"Lx_m_bl\", \"Lx_m_gr\", \"Lx_m_ndvi\", \"Lx_m_nir\", \"Lx_m_re\",\n",
    "                \"Lx_sd_bl\", \"Lx_sd_gr\", \"Lx_sd_ndvi\", \"Lx_sd_nir\", \"Lx_sd_re\",\n",
    "                \"Lx_t_diss\", \"Lx_t_hom\", \"Lx_t_mean\",\n",
    "                \"label\"]\n",
    "columnClass = [None] * 217 + [\"factor\", \"integer\"]\n",
    "\n",
    "# Import data\n",
    "preproc_DataPool = pd.read_csv(inputPath, header=0, sep=\";\", dtype=str, na_values=None)\n",
    "\n",
    "tmp_DataPool = preproc_DataPool.iloc[:, :-2]\n",
    "\n",
    "generalDataPool_columns = tmp_DataPool.columns\n",
    "\n",
    "converters = {col: lambda x: float(x.replace(',', '.')) for col in generalDataPool_columns}\n",
    "generalDataPool = pd.read_csv(inputPath, header=0, sep=\";\", na_values=None, converters=converters)\n",
    "\n",
    "\n",
    "generalDataPool.dropna(subset=[\"REF\"], inplace=True)  # Remove rows with missing REF values\n",
    "generalDataPool[\"REF\"] = pd.Categorical(generalDataPool[\"REF\"])\n",
    "\n",
    "# Transform to 2-Class-Case \"Bushes Trees\" VS rest\n",
    "first_label_class = generalDataPool[\"REF\"].cat.categories[0]  # Note that the first record is of class \"bushes trees\"\n",
    "generalDataPool[\"REF\"] = generalDataPool[\"REF\"].apply(lambda x: first_label_class if x == first_label_class else \"other\")\n",
    "generalDataPool[\"REF\"] = pd.Categorical(generalDataPool[\"REF\"])\n",
    "\n",
    "data = generalDataPool.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "REF = generalDataPool.iloc[:, -1]\n",
    "data_with_label = pd.concat([data, REF], axis=1)\n",
    "data_label = data_with_label.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L02_G_COMP     float64\n",
       "L02_G_EFIT     float64\n",
       "L02_G_RFIT     float64\n",
       "L02_G_ROUN     float64\n",
       "L02_G_SHIN     float64\n",
       "                ...   \n",
       "L13_T_DISS     float64\n",
       "L13_T_HOM      float64\n",
       "L13_T_MEA      float64\n",
       "REF           category\n",
       "USE              int64\n",
       "Length: 218, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalDataPool.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizedFeat['L02_G_COMP'] = normalizedFeat['L02_G_COMP'].replace(',', '.', regex=True)\n",
    "\n",
    "#normalizedFeat['L02_G_COMP'] = normalizedFeat['L02_G_COMP'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizedFeat['L02_G_EFIT'] = normalizedFeat['L02_G_EFIT'].apply(lambda x: pd.to_numeric(x.str.replace(',', '.'), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedFeat = generalDataPool.iloc[:, :-2]\n",
    "normalizedLabelUSE = generalDataPool.iloc[:, -2:]\n",
    "\n",
    "# Scaling\n",
    "preProc = MinMaxScaler()\n",
    "normalizedFeatBase = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]), columns=objInfoNames[:-1])\n",
    "\n",
    "# Apply range of basemodel to all levels\n",
    "normalizedFeat2 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, :numFeat]), columns=objInfoNames[:-1])\n",
    "normalizedFeat3 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, numFeat:(2 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat5 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (3 * numFeat):(4 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat6 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (4 * numFeat):(5 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat7 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (5 * numFeat):(6 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat8 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (6 * numFeat):(7 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat9 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (7 * numFeat):(8 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat10 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (8 * numFeat):(9 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat11 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (9 * numFeat):(10 * numFeat)]), columns=objInfoNames[:-1])\n",
    "\n",
    "# Recombine normalized sets to one data frame\n",
    "normalizedDataPoolAllLev = pd.concat([normalizedFeat2, normalizedFeat3, normalizedFeatBase, normalizedFeat5,\n",
    "                                      normalizedFeat6, normalizedFeat7, normalizedFeat8, normalizedFeat9,\n",
    "                                      normalizedFeat10, normalizedFeat11, normalizedLabelUSE], axis=1)\n",
    "\n",
    "# Remove used temporary variables\n",
    "del normalizedFeat, normalizedFeat2, normalizedFeat3, normalizedFeatBase, normalizedFeat5, normalizedFeat6\n",
    "del normalizedFeat7, normalizedFeat8, normalizedFeat9, normalizedFeat10, normalizedFeat11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test, train, and validate data\n",
    "trainDataPoolAllLev, testDataAllLev, validateDataAllLev = [df for _, df in normalizedDataPoolAllLev.groupby('USE')]\n",
    "trainDataPoolAllLev = trainDataPoolAllLev.iloc[:, :-1]\n",
    "testDataAllLev = testDataAllLev.iloc[:, :-1]\n",
    "validateFeatAllLev = validateDataAllLev.iloc[:, :-2]\n",
    "validateLabels = validateDataAllLev.iloc[:, -2]\n",
    "\n",
    "# Order train data pool by class label in alphabetical order\n",
    "trainDataPoolAllLev = trainDataPoolAllLev.sort_values(by=trainDataPoolAllLev.columns[-1])\n",
    "\n",
    "# Current training data-set, updated (refreshed) after each iteration\n",
    "trainDataCur = trainDataPoolAllLev.copy()\n",
    "testDataCur = testDataAllLev.copy()\n",
    "\n",
    "# Set randomized seed for the random sampling procedure\n",
    "seed = 5\n",
    "\n",
    "# Initial seed value for randomized sampling\n",
    "seed += np.random.randint(1, 101)\n",
    "\n",
    "# Definition of apriori-probabilities\n",
    "pA = pB = pC = pD = pE = pF = 1 / 6\n",
    "\n",
    "# Definition of training sample set sizes S [% of max. sample size]\n",
    "sCur = sMax * (sampleSizesPor[0] / 100)\n",
    "# Definition of sample shares\n",
    "nA, nB, nC, nD, nE, nF = [round(sCur * p) for p in [pA, pB, pC, pD, pE, pF]]\n",
    "shares = np.array([nA, nB, nC, nD, nE, nF])\n",
    "\n",
    "# Set randomized seed for the random sampling procedure\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REF'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validateDataAllLev.columns[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp = trainDataCur.groupby('REF', observed=False)[trainDataCur.columns].apply(lambda x: x.sample(67, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42210/1510426551.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stratSamp = trainDataCur.groupby('REF', observed=False).apply(sample_within_group)\n"
     ]
    }
   ],
   "source": [
    "# Define the sampling function\n",
    "def sample_within_group(group):\n",
    "    # Add the original IDs as a new column\n",
    "    group['ID_unit'] = group.index\n",
    "    \n",
    "    # Perform sampling within the group\n",
    "    sampled_group = group.sample(min(len(group), 67), replace=False)\n",
    "    \n",
    "    return sampled_group\n",
    "\n",
    "# Apply the sampling function to each group\n",
    "stratSamp = trainDataCur.groupby('REF', observed=False).apply(sample_within_group)\n",
    "\n",
    "# Reset the index to obtain a flat DataFrame with the original IDs preserved\n",
    "stratSamp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get samples of trainDataCur and set trainDataCur new\n",
    "trainDataCurRemaining = trainDataCur.drop(stratSamp[\"ID_unit\"])\n",
    "\n",
    "# Split test feat from test label for later join with trainData\n",
    "trainFeat = stratSamp.iloc[:, :len(trainDataPoolAllLev.columns)-1]\n",
    "trainLabels = stratSamp.iloc[:, len(trainDataPoolAllLev.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataPoolAllLev.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REF'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratSamp.columns[len(trainDataPoolAllLev.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp = trainDataCur.groupby('REF',observed=False)\n",
    "#stratSamp = stratSamp.apply(lambda x: x.sample(67, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of sampling configuration (strata: random sampling without replacement)\n",
    "#stratSamp = trainDataCur.groupby('REF').apply(lambda x: x.sample(shares, replace=False)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lx_t_diss'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataCur.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratSamp.iloc[:,181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratSamp[\"ID_unit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeat.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42210/3587032453.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stratSamp = testDataCur.groupby('REF', observed=False).apply(sample_within_group)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subset for each outer iteration test data to speed up computing\n",
    "testDataCur = testDataCur.sort_values(by=testDataCur.columns[-1])\n",
    "# Apply the sampling function to each group\n",
    "stratSamp = testDataCur.groupby('REF', observed=False).apply(sample_within_group)\n",
    "\n",
    "# Split test feat from test label for later join with trainData\n",
    "testFeat = stratSamp.iloc[:, :len(testDataCur.columns)-1]\n",
    "testLabels = stratSamp.iloc[:, len(testDataCur.columns)-1]\n",
    "\n",
    "# Subset on base level\n",
    "testFeatsub = testFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# TrainData index to split between train and test in svmFit\n",
    "countTrainData = trainFeat.shape[0]\n",
    "indexTrainData = [list(range(1, countTrainData + 1))]\n",
    "\n",
    "# SVM base for invariants\n",
    "\n",
    "# Subset on L_4\n",
    "trainFeat = trainFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# Join train and test data (separable through indexTrainData in svmFit)\n",
    "tuneFeat = pd.concat([trainFeat, testFeatsub], axis=0)\n",
    "tuneLabel = np.concatenate((trainLabels.values, testLabels.values))\n",
    "\n",
    "validateFeatsub = validateFeatAllLev.iloc[:, sindexSVMDATA:eindexSVMDATA+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         'gamma': array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "# Expand coarse grid\n",
    "coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "\n",
    "\n",
    "# Coarse grid search\n",
    "svm_coarse = SVC(kernel='rbf')\n",
    "svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring='accuracy')#, cv=indexTrainData)\n",
    "svm_coarse_cv.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuneFeat.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuneLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         'gamma': array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand coarse grid\n",
    "coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "\n",
    "# Coarse grid search\n",
    "svm_coarse = SVC(kernel='rbf')\n",
    "svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring='accuracy')#, cv=indexTrainData)\n",
    "svm_coarse_cv.fit(tuneFeat, tuneLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([ 4.        ,  5.65685425,  8.        , 11.3137085 , 16.        ,\n",
       "       22.627417  , 32.        , 45.254834  ]),\n",
       "                         &#x27;gamma&#x27;: array([0.125     , 0.1767767 , 0.25      , 0.35355339, 0.5       ,\n",
       "       0.70710678, 1.        , 1.41421356])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([ 4.        ,  5.65685425,  8.        , 11.3137085 , 16.        ,\n",
       "       22.627417  , 32.        , 45.254834  ]),\n",
       "                         &#x27;gamma&#x27;: array([0.125     , 0.1767767 , 0.25      , 0.35355339, 0.5       ,\n",
       "       0.70710678, 1.        , 1.41421356])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([ 4.        ,  5.65685425,  8.        , 11.3137085 , 16.        ,\n",
       "       22.627417  , 32.        , 45.254834  ]),\n",
       "                         'gamma': array([0.125     , 0.1767767 , 0.25      , 0.35355339, 0.5       ,\n",
       "       0.70710678, 1.        , 1.41421356])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best coarse grid parameters\n",
    "best_c = svm_coarse_cv.best_params_['C']\n",
    "best_gamma = svm_coarse_cv.best_params_['gamma']\n",
    "\n",
    "# Define narrow grid borders\n",
    "a_gamma = np.log2(best_gamma) - 2\n",
    "b_gamma = np.log2(best_gamma) + 2\n",
    "a_c = np.log2(best_c) - 2\n",
    "b_c = np.log2(best_c) + 2\n",
    "\n",
    "# Expand narrow grid\n",
    "narrow_grid = {'C': 2.0 ** np.arange(a_c, b_c, 0.5),\n",
    "                'gamma': 2.0 ** np.arange(a_gamma, b_gamma, 0.5)}\n",
    "\n",
    "# Narrow grid search\n",
    "svm_narrow = SVC(kernel='rbf')\n",
    "svm_narrow_cv = GridSearchCV(svm_narrow, param_grid=narrow_grid, scoring='accuracy')#, cv=indexTrainData)\n",
    "svm_narrow_cv.fit(tuneFeat, tuneLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM parameter tuning\n",
    "svm_model=svm_fit(trainFeat, trainLabels, indexTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validateLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8637477761810616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict labels of test data\n",
    "predLabelsSVM = svm_model.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accSVM = accuracy_score(validateLabels, predLabelsSVM)\n",
    "print(accSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSVM on all Level SV\n",
    "SVindex = svm_model.best_estimator_.support_  # Indices of support vectors\n",
    "SVtotal = trainDataCur.iloc[SVindex, sindexSVMDATA:eindexSVMDATA+1].reset_index(drop=True)  # Get support vectors\n",
    "\n",
    "SVL2 = trainDataCur.iloc[SVindex, sindexSVMDATA - 2 * numFeat:sindexSVMDATA - numFeat ].reset_index(drop=True)\n",
    "SVL3 = trainDataCur.iloc[SVindex, sindexSVMDATA - numFeat:sindexSVMDATA ].reset_index(drop=True)\n",
    "\n",
    "SVL5 = trainDataCur.iloc[SVindex, sindexSVMDATA + numFeat:sindexSVMDATA + 2 * numFeat ].reset_index(drop=True)\n",
    "SVL6 = trainDataCur.iloc[SVindex, sindexSVMDATA + 2 * numFeat:sindexSVMDATA + 3 * numFeat ].reset_index(drop=True)\n",
    "SVL7 = trainDataCur.iloc[SVindex, sindexSVMDATA + 3 * numFeat:sindexSVMDATA + 4 * numFeat ].reset_index(drop=True)\n",
    "SVL8 = trainDataCur.iloc[SVindex, sindexSVMDATA + 4 * numFeat:sindexSVMDATA + 5 * numFeat ].reset_index(drop=True)\n",
    "SVL9 = trainDataCur.iloc[SVindex, sindexSVMDATA + 5 * numFeat:sindexSVMDATA + 6 * numFeat ].reset_index(drop=True)\n",
    "SVL10 = trainDataCur.iloc[SVindex, sindexSVMDATA + 6 * numFeat:sindexSVMDATA + 7 * numFeat ].reset_index(drop=True)\n",
    "SVL11 = trainDataCur.iloc[SVindex, sindexSVMDATA + 7 * numFeat:sindexSVMDATA + 8 * numFeat ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Bind original SV with modified to new train data set\n",
    "SVinvar = pd.concat([SVtotal, SVL2, SVL3, SVL5, SVL6, SVL7, SVL8, SVL9, SVL10, SVL11],ignore_index=True)\n",
    "\n",
    "# Split for training to feature and label\n",
    "trainFeatVSVM = SVinvar.iloc[:, :-1].reset_index(drop=True)\n",
    "trainLabelsVSVM = SVinvar.iloc[:, -1]\n",
    "\n",
    "# Get list with index of train data to split between train and test in svmFit\n",
    "countTrainData = SVinvar.shape[0]\n",
    "indexTrainData = [list(range(1, countTrainData + 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join of train and test test data (through indexTrainData in svmFit separable)\n",
    "tuneFeatVSVM = pd.concat([trainFeatVSVM, testFeatsub])\n",
    "tuneLabelsVSVM = np.concatenate((trainLabelsVSVM.values, testLabels.values))\n",
    "\n",
    "# VSVM parameter tuning\n",
    "tunedVSVM = svm_fit(tuneFeatVSVM, tuneLabelsVSVM)\n",
    "\n",
    "# Run classification and accuracy assessment for modified SV\n",
    "predLabelsVSVM = tunedVSVM.predict(validateFeatsub)\n",
    "accVSVM = accuracy_score(validateLabels, predLabelsVSVM)\n",
    "print(accVSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(trainFeatVSVM, testFeatsub, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lx_g_comp', 'Lx_g_elfi', 'Lx_g_refi', 'Lx_g_roun', 'Lx_g_shin',\n",
       "       'Lx_m_bl', 'Lx_m_gr', 'Lx_m_ndvi', 'Lx_m_nir', 'Lx_m_re', 'Lx_sd_bl',\n",
       "       'Lx_sd_gr', 'Lx_sd_ndvi', 'Lx_sd_nir', 'Lx_sd_re', 'Lx_t_diss',\n",
       "       'Lx_t_hom'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeatVSVM.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lx_g_comp', 'Lx_g_elfi', 'Lx_g_refi', 'Lx_g_roun', 'Lx_g_shin',\n",
       "       'Lx_m_bl', 'Lx_m_gr', 'Lx_m_ndvi', 'Lx_m_nir', 'Lx_m_re', 'Lx_sd_bl',\n",
       "       'Lx_sd_gr', 'Lx_sd_ndvi', 'Lx_sd_nir', 'Lx_sd_re', 'Lx_t_diss',\n",
       "       'Lx_t_hom', 'Lx_t_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFeatsub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.529954</td>\n",
       "      <td>0.755812</td>\n",
       "      <td>0.216550</td>\n",
       "      <td>0.217151</td>\n",
       "      <td>0.043132</td>\n",
       "      <td>0.042229</td>\n",
       "      <td>0.524556</td>\n",
       "      <td>0.084285</td>\n",
       "      <td>0.045613</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.063318</td>\n",
       "      <td>0.380959</td>\n",
       "      <td>0.161381</td>\n",
       "      <td>0.061692</td>\n",
       "      <td>0.142263</td>\n",
       "      <td>0.033707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.805668</td>\n",
       "      <td>0.873817</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>0.092888</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.032189</td>\n",
       "      <td>0.620288</td>\n",
       "      <td>0.100116</td>\n",
       "      <td>0.036525</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.405775</td>\n",
       "      <td>0.165309</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.090419</td>\n",
       "      <td>0.052963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.805668</td>\n",
       "      <td>0.873817</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>0.092888</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.032189</td>\n",
       "      <td>0.620288</td>\n",
       "      <td>0.100116</td>\n",
       "      <td>0.036525</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.405775</td>\n",
       "      <td>0.165309</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.090419</td>\n",
       "      <td>0.052963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl   Lx_m_gr  \\\n",
       "0   0.095265   0.529954   0.755812   0.216550   0.217151  0.043132  0.042229   \n",
       "1   0.068239   0.805668   0.873817   0.141781   0.092888  0.033785  0.032189   \n",
       "2   0.068239   0.805668   0.873817   0.141781   0.092888  0.033785  0.032189   \n",
       "3   0.073022   0.670330   0.798266   0.201985   0.077653  0.021294  0.014574   \n",
       "4   0.073022   0.670330   0.798266   0.201985   0.077653  0.021294  0.014574   \n",
       "5   0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "6   0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "7   0.073022   0.670330   0.798266   0.201985   0.077653  0.021294  0.014574   \n",
       "8   0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "9   0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "\n",
       "   Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  \\\n",
       "0   0.524556  0.084285  0.045613  0.061682  0.063318    0.380959   0.161381   \n",
       "1   0.620288  0.100116  0.036525  0.040698  0.037860    0.405775   0.165309   \n",
       "2   0.620288  0.100116  0.036525  0.040698  0.037860    0.405775   0.165309   \n",
       "3   0.631935  0.061670  0.007359  0.026509  0.022434    0.316412   0.119451   \n",
       "4   0.631935  0.061670  0.007359  0.026509  0.022434    0.316412   0.119451   \n",
       "5   0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "6   0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "7   0.631935  0.061670  0.007359  0.026509  0.022434    0.316412   0.119451   \n",
       "8   0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "9   0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "\n",
       "   Lx_sd_re  Lx_t_diss  Lx_t_hom  \n",
       "0  0.061692   0.142263  0.033707  \n",
       "1  0.044026   0.090419  0.052963  \n",
       "2  0.044026   0.090419  0.052963  \n",
       "3  0.020073   0.098026  0.060738  \n",
       "4  0.020073   0.098026  0.060738  \n",
       "5  0.030528   0.124826  0.042560  \n",
       "6  0.030528   0.124826  0.042560  \n",
       "7  0.020073   0.098026  0.060738  \n",
       "8  0.159947   0.146974  0.050292  \n",
       "9  0.159947   0.146974  0.050292  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeatVSVM[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170638</td>\n",
       "      <td>0.175141</td>\n",
       "      <td>0.547923</td>\n",
       "      <td>0.373950</td>\n",
       "      <td>0.335203</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.633638</td>\n",
       "      <td>0.088880</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.054977</td>\n",
       "      <td>0.048150</td>\n",
       "      <td>0.384791</td>\n",
       "      <td>0.177123</td>\n",
       "      <td>0.037086</td>\n",
       "      <td>0.140759</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>0.779395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053001</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.857845</td>\n",
       "      <td>0.143951</td>\n",
       "      <td>0.106342</td>\n",
       "      <td>0.028123</td>\n",
       "      <td>0.020053</td>\n",
       "      <td>0.479045</td>\n",
       "      <td>0.046451</td>\n",
       "      <td>0.019651</td>\n",
       "      <td>0.036058</td>\n",
       "      <td>0.027941</td>\n",
       "      <td>0.613459</td>\n",
       "      <td>0.134085</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.058016</td>\n",
       "      <td>0.800984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049515</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.887633</td>\n",
       "      <td>0.016153</td>\n",
       "      <td>0.049614</td>\n",
       "      <td>0.040455</td>\n",
       "      <td>0.037858</td>\n",
       "      <td>0.665125</td>\n",
       "      <td>0.118128</td>\n",
       "      <td>0.038382</td>\n",
       "      <td>0.125921</td>\n",
       "      <td>0.114288</td>\n",
       "      <td>0.801519</td>\n",
       "      <td>0.394055</td>\n",
       "      <td>0.118582</td>\n",
       "      <td>0.122787</td>\n",
       "      <td>0.046085</td>\n",
       "      <td>0.793211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134584</td>\n",
       "      <td>0.642276</td>\n",
       "      <td>0.793205</td>\n",
       "      <td>0.260702</td>\n",
       "      <td>0.123789</td>\n",
       "      <td>0.032374</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.546970</td>\n",
       "      <td>0.069121</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>0.053448</td>\n",
       "      <td>0.043370</td>\n",
       "      <td>0.399808</td>\n",
       "      <td>0.149164</td>\n",
       "      <td>0.052509</td>\n",
       "      <td>0.146873</td>\n",
       "      <td>0.035488</td>\n",
       "      <td>0.788992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037543</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.890626</td>\n",
       "      <td>0.063433</td>\n",
       "      <td>0.036538</td>\n",
       "      <td>0.040562</td>\n",
       "      <td>0.038807</td>\n",
       "      <td>0.799970</td>\n",
       "      <td>0.170582</td>\n",
       "      <td>0.033094</td>\n",
       "      <td>0.110431</td>\n",
       "      <td>0.083997</td>\n",
       "      <td>0.422837</td>\n",
       "      <td>0.317054</td>\n",
       "      <td>0.072970</td>\n",
       "      <td>0.167546</td>\n",
       "      <td>0.015673</td>\n",
       "      <td>0.775228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.057376</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.830359</td>\n",
       "      <td>0.110736</td>\n",
       "      <td>0.054884</td>\n",
       "      <td>0.046727</td>\n",
       "      <td>0.043753</td>\n",
       "      <td>0.700948</td>\n",
       "      <td>0.145417</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>0.064467</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>0.383194</td>\n",
       "      <td>0.237479</td>\n",
       "      <td>0.069540</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.032199</td>\n",
       "      <td>0.780111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.128324</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.759943</td>\n",
       "      <td>0.245565</td>\n",
       "      <td>0.156693</td>\n",
       "      <td>0.051341</td>\n",
       "      <td>0.061375</td>\n",
       "      <td>0.514011</td>\n",
       "      <td>0.104413</td>\n",
       "      <td>0.066220</td>\n",
       "      <td>0.155476</td>\n",
       "      <td>0.138538</td>\n",
       "      <td>0.449892</td>\n",
       "      <td>0.146757</td>\n",
       "      <td>0.136889</td>\n",
       "      <td>0.262844</td>\n",
       "      <td>0.024569</td>\n",
       "      <td>0.819940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.057376</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.830359</td>\n",
       "      <td>0.110736</td>\n",
       "      <td>0.054884</td>\n",
       "      <td>0.046727</td>\n",
       "      <td>0.043753</td>\n",
       "      <td>0.700948</td>\n",
       "      <td>0.145417</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>0.064467</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>0.383194</td>\n",
       "      <td>0.237479</td>\n",
       "      <td>0.069540</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.032199</td>\n",
       "      <td>0.780111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.110442</td>\n",
       "      <td>0.448889</td>\n",
       "      <td>0.694736</td>\n",
       "      <td>0.318567</td>\n",
       "      <td>0.263296</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.568207</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>0.014487</td>\n",
       "      <td>0.055461</td>\n",
       "      <td>0.054716</td>\n",
       "      <td>0.474513</td>\n",
       "      <td>0.154968</td>\n",
       "      <td>0.068994</td>\n",
       "      <td>0.145481</td>\n",
       "      <td>0.044087</td>\n",
       "      <td>0.826634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063021</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>0.150798</td>\n",
       "      <td>0.159881</td>\n",
       "      <td>0.040094</td>\n",
       "      <td>0.037353</td>\n",
       "      <td>0.554337</td>\n",
       "      <td>0.084519</td>\n",
       "      <td>0.039136</td>\n",
       "      <td>0.051960</td>\n",
       "      <td>0.049767</td>\n",
       "      <td>0.458626</td>\n",
       "      <td>0.210503</td>\n",
       "      <td>0.053680</td>\n",
       "      <td>0.100770</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>0.788544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl   Lx_m_gr  \\\n",
       "0   0.170638   0.175141   0.547923   0.373950   0.335203  0.035360  0.029126   \n",
       "1   0.053001   0.720588   0.857845   0.143951   0.106342  0.028123  0.020053   \n",
       "2   0.049515   0.890909   0.887633   0.016153   0.049614  0.040455  0.037858   \n",
       "3   0.134584   0.642276   0.793205   0.260702   0.123789  0.032374  0.026021   \n",
       "4   0.037543   0.714286   0.890626   0.063433   0.036538  0.040562  0.038807   \n",
       "5   0.057376   0.735849   0.830359   0.110736   0.054884  0.046727  0.043753   \n",
       "6   0.128324   0.538462   0.759943   0.245565   0.156693  0.051341  0.061375   \n",
       "7   0.057376   0.735849   0.830359   0.110736   0.054884  0.046727  0.043753   \n",
       "8   0.110442   0.448889   0.694736   0.318567   0.263296  0.018281  0.013568   \n",
       "9   0.063021   0.684211   0.825306   0.150798   0.159881  0.040094  0.037353   \n",
       "\n",
       "   Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  \\\n",
       "0   0.633638  0.088880  0.025840  0.054977  0.048150    0.384791   0.177123   \n",
       "1   0.479045  0.046451  0.019651  0.036058  0.027941    0.613459   0.134085   \n",
       "2   0.665125  0.118128  0.038382  0.125921  0.114288    0.801519   0.394055   \n",
       "3   0.546970  0.069121  0.027834  0.053448  0.043370    0.399808   0.149164   \n",
       "4   0.799970  0.170582  0.033094  0.110431  0.083997    0.422837   0.317054   \n",
       "5   0.700948  0.145417  0.046122  0.064467  0.053819    0.383194   0.237479   \n",
       "6   0.514011  0.104413  0.066220  0.155476  0.138538    0.449892   0.146757   \n",
       "7   0.700948  0.145417  0.046122  0.064467  0.053819    0.383194   0.237479   \n",
       "8   0.568207  0.057481  0.014487  0.055461  0.054716    0.474513   0.154968   \n",
       "9   0.554337  0.084519  0.039136  0.051960  0.049767    0.458626   0.210503   \n",
       "\n",
       "   Lx_sd_re  Lx_t_diss  Lx_t_hom  Lx_t_mean  \n",
       "0  0.037086   0.140759  0.033392   0.779395  \n",
       "1  0.032637   0.106267  0.058016   0.800984  \n",
       "2  0.118582   0.122787  0.046085   0.793211  \n",
       "3  0.052509   0.146873  0.035488   0.788992  \n",
       "4  0.072970   0.167546  0.015673   0.775228  \n",
       "5  0.069540   0.157800  0.032199   0.780111  \n",
       "6  0.136889   0.262844  0.024569   0.819940  \n",
       "7  0.069540   0.157800  0.032199   0.780111  \n",
       "8  0.068994   0.145481  0.044087   0.826634  \n",
       "9  0.053680   0.100770  0.058101   0.788544  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFeatsub[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.785337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.785337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "      <td>0.803718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.128719</td>\n",
       "      <td>0.450644</td>\n",
       "      <td>0.696731</td>\n",
       "      <td>0.415129</td>\n",
       "      <td>0.209054</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.046634</td>\n",
       "      <td>0.732685</td>\n",
       "      <td>0.160815</td>\n",
       "      <td>0.046254</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.036228</td>\n",
       "      <td>0.225737</td>\n",
       "      <td>0.110220</td>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.192499</td>\n",
       "      <td>0.038910</td>\n",
       "      <td>0.788608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.893459</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.130733</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.807497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.893459</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.130733</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.807497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.893459</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.130733</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.807497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.091831</td>\n",
       "      <td>0.693431</td>\n",
       "      <td>0.830558</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.105103</td>\n",
       "      <td>0.025162</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.604674</td>\n",
       "      <td>0.088022</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.041283</td>\n",
       "      <td>0.034824</td>\n",
       "      <td>0.108085</td>\n",
       "      <td>0.078686</td>\n",
       "      <td>0.036023</td>\n",
       "      <td>0.262832</td>\n",
       "      <td>0.028123</td>\n",
       "      <td>0.822724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl   Lx_m_gr  \\\n",
       "0    0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "1    0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "2    0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "3    0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "4    0.073022   0.670330   0.798266   0.201985   0.077653  0.021294  0.014574   \n",
       "..        ...        ...        ...        ...        ...       ...       ...   \n",
       "84   0.128719   0.450644   0.696731   0.415129   0.209054  0.033694  0.046634   \n",
       "85   0.048208   0.821429   0.893459   0.113375   0.126157  0.033787  0.043146   \n",
       "86   0.048208   0.821429   0.893459   0.113375   0.126157  0.033787  0.043146   \n",
       "87   0.048208   0.821429   0.893459   0.113375   0.126157  0.033787  0.043146   \n",
       "88   0.091831   0.693431   0.830558   0.193400   0.105103  0.025162  0.027748   \n",
       "\n",
       "    Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  \\\n",
       "0    0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "1    0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "2    0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "3    0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "4    0.631935  0.061670  0.007359  0.026509  0.022434    0.316412   0.119451   \n",
       "..        ...       ...       ...       ...       ...         ...        ...   \n",
       "84   0.732685  0.160815  0.046254  0.042091  0.036228    0.225737   0.110220   \n",
       "85   0.646950  0.127958  0.049292  0.047404  0.037481    0.187865   0.130733   \n",
       "86   0.646950  0.127958  0.049292  0.047404  0.037481    0.187865   0.130733   \n",
       "87   0.646950  0.127958  0.049292  0.047404  0.037481    0.187865   0.130733   \n",
       "88   0.604674  0.088022  0.031135  0.041283  0.034824    0.108085   0.078686   \n",
       "\n",
       "    Lx_sd_re  Lx_t_diss  Lx_t_hom  Lx_t_mean  \n",
       "0   0.030528   0.124826  0.042560   0.785337  \n",
       "1   0.030528   0.124826  0.042560   0.785337  \n",
       "2   0.159947   0.146974  0.050292   0.826420  \n",
       "3   0.159947   0.146974  0.050292   0.826420  \n",
       "4   0.020073   0.098026  0.060738   0.803718  \n",
       "..       ...        ...       ...        ...  \n",
       "84  0.048571   0.192499  0.038910   0.788608  \n",
       "85  0.036498   0.197834  0.042960   0.807497  \n",
       "86  0.036498   0.197834  0.042960   0.807497  \n",
       "87  0.036498   0.197834  0.042960   0.807497  \n",
       "88  0.036023   0.262832  0.028123   0.822724  \n",
       "\n",
       "[267 rows x 18 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([SVtotal,SVL2,SVL5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# VSVM - EVALUATION of all Level VSV\n",
    "actKappa = 0\n",
    "bestFittingModel = None\n",
    "\n",
    "# Iteration over bound to test different bound thresholds determining the radius of acceptance\n",
    "for jj, bound_val in enumerate(bound, start=1):\n",
    "    SVinvarRadi_list = []\n",
    "    \n",
    "    # Iterating over boundMargin to test different thresholds on margin distance\n",
    "    for kk, bound_margin_val in enumerate(boundMargin, start=1):\n",
    "        SVinvar_list = []\n",
    "        \n",
    "        # Iterate over SVinvarRadi and evaluate distance to hyperplane\n",
    "        for m in range(len(SVinvarRadi)):\n",
    "            signa = pred_one(tunedSVM.finalModel, SVinvarRadi[m, :-1])\n",
    "            if SVinvarRadi[m, -1] == levels(generalDataPool.REF)[0]:\n",
    "                if -bound_margin_val < signa < bound_margin_val:\n",
    "                    SVinvar_list.append(SVinvarRadi[m, :])\n",
    "            else:\n",
    "                if -bound_margin_val < signa < bound_margin_val:\n",
    "                    SVinvar_list.append(SVinvarRadi[m, :])\n",
    "\n",
    "        SVinvar = pd.DataFrame(SVinvar_list, columns=objInfoNames)\n",
    "        \n",
    "        # Merge elected VSV with original SV\n",
    "        SVinvar_org = pd.concat([SVtotal, SVinvar])\n",
    "\n",
    "        # Split for training to feature and label\n",
    "        trainFeatVSVM = SVinvar_org.iloc[:, :-1]\n",
    "        trainLabelsVSVM = SVinvar_org.iloc[:, -1]\n",
    "\n",
    "        # Get list with index of trainData to split between train and test in svmFit\n",
    "        countTrainData = SVinvar_org.shape[0]\n",
    "        indexTrainData = [list(range(1, countTrainData + 1))]\n",
    "\n",
    "        # Join of train and test data (through indexTrainData in svmFit separable)\n",
    "        names = objInfoNames[:-1]\n",
    "        tuneFeatVSVM = pd.concat([trainFeatVSVM, testFeatsub], axis=0)\n",
    "        tuneFeatVSVM.columns = names\n",
    "        tuneLabelsVSVM = np.concatenate((trainLabelsVSVM.values, testLabels.values))\n",
    "\n",
    "        ######################################## VSVM control parameter tuning ########################################\n",
    "        tunedVSVM = SVC(kernel='linear')\n",
    "        tunedVSVM.fit(tuneFeatVSVM, tuneLabelsVSVM)\n",
    "\n",
    "        # Get the best fitting model based on Kappa\n",
    "        if actKappa < tunedVSVM.resample.Kappa:\n",
    "            bestFittingModel = tunedVSVM\n",
    "            actKappa = tunedVSVM.resample.Kappa\n",
    "\n",
    "# Run classification and accuracy assessment for the best bound setting\n",
    "# Predict labels of test data\n",
    "predLabelsVSVMsum = bestFittingModel.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL = accuracy_score(validateLabels, predLabelsVSVMsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Balanced & Random unlabeled samples\n",
    "# Balanced samples\n",
    "\n",
    "# Definition of sampling configuration (strata: random sampling without replacement)\n",
    "stratSampRemaining_b = resample(trainDataCurRemaining, n_samples=[b, b, b, b, b, b], replace=False)\n",
    "samplesRemaining_b = trainDataCurRemaining.iloc[stratSampRemaining_b]\n",
    "\n",
    "trainDataCurRemaining_b = samplesRemaining_b.iloc[:, :-1]\n",
    "trainDataCurRemainingsub_b = trainDataCurRemaining_b.iloc[:, sindexSVMDATA:eindexSVMDATA]\n",
    "REF_b = bestFittingModel.predict(trainDataCurRemainingsub_b)\n",
    "\n",
    "SVindexUn_b = np.arange(1, len(trainDataCurRemainingsub_b) + 1)\n",
    "SVtotalUn_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA:eindexSVMDATA]\n",
    "SVtotalUn_b['REF'] = REF_b\n",
    "\n",
    "SVL2Un_b = trainDataCurRemaining.iloc[SVindexUn_b - 1, sindexSVMDATA - 2*numFeat:sindexSVMDATA - numFeat - 1].copy()\n",
    "SVL2Un_b['REF'] = REF_b\n",
    "SVL3Un_b = trainDataCurRemaining.iloc[SVindexUn_b - 1, sindexSVMDATA - numFeat:sindexSVMDATA - 1].copy()\n",
    "SVL3Un_b['REF'] = REF_b\n",
    "SVL5Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + numFeat:sindexSVMDATA + 2*numFeat - 1].copy()\n",
    "SVL5Un_b['REF'] = REF_b\n",
    "SVL6Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 2*numFeat:sindexSVMDATA + 3*numFeat - 1].copy()\n",
    "SVL6Un_b['REF'] = REF_b\n",
    "SVL7Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 3*numFeat:sindexSVMDATA + 4*numFeat - 1].copy()\n",
    "SVL7Un_b['REF'] = REF_b\n",
    "SVL8Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 4*numFeat:sindexSVMDATA + 5*numFeat - 1].copy()\n",
    "SVL8Un_b['REF'] = REF_b\n",
    "SVL9Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 5*numFeat:sindexSVMDATA + 6*numFeat - 1].copy()\n",
    "SVL9Un_b['REF'] = REF_b\n",
    "SVL10Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 6*numFeat:sindexSVMDATA + 7*numFeat - 1].copy()\n",
    "SVL10Un_b['REF'] = REF_b\n",
    "SVL11Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 7*numFeat:sindexSVMDATA + 8*numFeat - 1].copy()\n",
    "SVL11Un_b['REF'] = REF_b\n",
    "\n",
    "SVinvarUn_b = pd.concat([SVtotalUn_b, SVL2Un_b, SVL3Un_b, SVL5Un_b, SVL6Un_b, SVL7Un_b, SVL8Un_b, SVL9Un_b, SVL10Un_b, SVL11Un_b])\n",
    "\n",
    "# Balanced Unlabeled samples\n",
    "\n",
    "actKappa = 0\n",
    "\n",
    "for jj in range(len(bound)):\n",
    "    SVinvarRadiUn_b_list = []\n",
    "    for m in range(len(SVinvarRadiUn_b)):\n",
    "        signa = pred_one(tunedSVM.finalModel, SVinvarRadiUn_b.iloc[m, :-1])\n",
    "        if SVinvarRadiUn_b.iloc[m, -1] == levels(generalDataPool.REF)[0]:\n",
    "            if -bound_margin_val < signa < bound_margin_val:\n",
    "                SVinvarRadiUn_b_list.append(SVinvarRadiUn_b.iloc[m, :])\n",
    "        else:\n",
    "            if -bound_margin_val < signa < bound_margin_val:\n",
    "                SVinvarRadiUn_b_list.append(SVinvarRadiUn_b.iloc[m, :])\n",
    "\n",
    "    SVinvarUn_b = pd.DataFrame(SVinvarRadiUn_b_list, columns=objInfoNames)\n",
    "    \n",
    "    SVinvar_orgUn_b = pd.concat([SVtotal, SVinvarUn_b])\n",
    "\n",
    "    trainFeatVSVMUn_b = SVinvar_orgUn_b.iloc[:, :-1]\n",
    "    trainLabelsVSVMUn_b = SVinvar_orgUn_b.iloc[:, -1]\n",
    "\n",
    "    countTrainDataUn_b = SVinvar_orgUn_b.shape[0]\n",
    "    indexTrainDataUn_b = [list(range(1, countTrainDataUn_b + 1))]\n",
    "\n",
    "    names = objInfoNames[:-1]\n",
    "    tuneFeatVSVMUn_b = pd.concat([trainFeatVSVMUn_b, testFeatsub], axis=0)\n",
    "    tuneFeatVSVMUn_b.columns = names\n",
    "    tuneLabelsVSVMUn_b = np.concatenate((trainLabelsVSVMUn_b.values, testLabels.values))\n",
    "\n",
    "    tunedVSVMUn_b = SVC(kernel='linear')\n",
    "    tunedVSVMUn_b.fit(tuneFeatVSVMUn_b, tuneLabelsVSVMUn_b)\n",
    "\n",
    "    if actKappa < tunedVSVMUn_b.resample.Kappa:\n",
    "        bestFittingModelUn_b = tunedVSVMUn_b\n",
    "        actKappa = tunedVSVMUn_b.resample.Kappa\n",
    "\n",
    "# Run classification and accuracy assessment for the best bound setting\n",
    "predLabelsVSVMsumUn_b = bestFittingModelUn_b.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL_Un_b = accuracy_score(validateLabels, predLabelsVSVMsumUn_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted labels to the features data set\n",
    "predLabelsVSVMsumUn_unc = pd.concat([validateFeatsub, pd.DataFrame(predLabelsVSVMsumUn_b, columns=[\"Predicted_Labels\"])], axis=1)\n",
    "predLabelsVSVMsumUn_unc.columns = objInfoNames\n",
    "\n",
    "# Calculate uncertainty of the samples by selecting SV's and data set\n",
    "normdistvsvm_sl_un = uncertainty_dist_v2_2(bestFittingModelUn_b, predLabelsVSVMsumUn_unc)\n",
    "\n",
    "# Alter labels\n",
    "predlabels_vsvm_Slu = alter_labels(normdistvsvm_sl_un, validateLabels)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL_Un_b_ad = accuracy_score(validateLabels, predlabels_vsvm_Slu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#from dfply import arrange\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path1 = \"D:/tunc_oz/apply_model\"\n",
    "path1 = \"/home/rsrg9/Documents/tunc_oz/apply_model\"\n",
    "os.chdir(path1)\n",
    "\n",
    "# Second path\n",
    "path2 = \"csv_data_r_import/cologne/scale\"\n",
    "os.chdir(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(x, y, index_train=None):\n",
    "    # Expand coarse grid\n",
    "    coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                   'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "    \n",
    "    kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "    # Coarse grid search\n",
    "    svm_coarse = SVC(kernel='rbf')\n",
    "    svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring=kappa_scorer)#, cv=index_train)\n",
    "    svm_coarse_cv.fit(x, y)\n",
    "    \n",
    "    # Get best coarse grid parameters\n",
    "    best_c = svm_coarse_cv.best_params_['C']\n",
    "    best_gamma = svm_coarse_cv.best_params_['gamma']\n",
    "    \n",
    "    # Define narrow grid borders\n",
    "    a_gamma = np.log2(best_gamma) - 2\n",
    "    b_gamma = np.log2(best_gamma) + 2\n",
    "    a_c = np.log2(best_c) - 2\n",
    "    b_c = np.log2(best_c) + 2\n",
    "    \n",
    "    # Expand narrow grid\n",
    "    narrow_grid = {'C': 2.0 ** np.arange(a_c, b_c, 0.5),\n",
    "                   'gamma': 2.0 ** np.arange(a_gamma, b_gamma, 0.5)}\n",
    "    \n",
    "    # Narrow grid search\n",
    "    svm_narrow = SVC(kernel='rbf')\n",
    "    svm_narrow_cv = GridSearchCV(svm_narrow, param_grid=narrow_grid, scoring=kappa_scorer)#, cv=index_train)\n",
    "    svm_narrow_cv.fit(x, y)\n",
    "    \n",
    "    return svm_narrow_cv\n",
    "\n",
    "# Usage example:\n",
    "# svm_model = svm_fit(X_train, y_train, StratifiedKFold(n_splits=10, shuffle=True))\n",
    "\n",
    "# Euclidean Distance between two points lying in the input space\n",
    "def euc_dis(a, b):\n",
    "    temp = 0\n",
    "    for ii in range(len(a)):\n",
    "        temp += (a[ii] - b[ii])**2\n",
    "    return np.sqrt(temp)\n",
    "\n",
    "# Evaluate the distance between Virtual Support Vectors and Support Vectors lying in the input space\n",
    "def rem_extrem(org, VSV1, a):\n",
    "    distance = pd.DataFrame(index=range(len(org)), columns=['label', 'distance'])\n",
    "    distanceSVC1 = []\n",
    "    distanceSVC2 = []\n",
    "    \n",
    "    for l in range(len(org)):\n",
    "        distance.loc[l, 'label'] = str(org.iloc[l, -1])\n",
    "        distance.loc[l, 'distance'] = euc_dis(org.iloc[l, :-1], VSV1.iloc[l, :-1])\n",
    "    \n",
    "    SVClass1 = org[org['REF'] == org['REF'].unique()[0]]\n",
    "    SVClass2 = org[org['REF'] == org['REF'].unique()[1]]\n",
    "    \n",
    "    if len(SVClass1) > 0:\n",
    "        for n in range(len(SVClass1) - 1):\n",
    "            for nn in range(n, len(SVClass1) - 1):\n",
    "                distanceSVC1.append(euc_dis(SVClass1.iloc[n, :-1], SVClass1.iloc[n + nn, :-1]))\n",
    "        disClass1median = np.mean(distanceSVC1)\n",
    "        boundClass1 = disClass1median * a\n",
    "    \n",
    "    if len(SVClass2) > 0:\n",
    "        for n in range(len(SVClass2) - 1):\n",
    "            for nn in range(n, len(SVClass2) - 1):\n",
    "                distanceSVC2.append(euc_dis(SVClass2.iloc[n, :-1], SVClass2.iloc[n + nn, :-1]))\n",
    "        disClass2median = np.mean(distanceSVC2)\n",
    "        boundClass2 = disClass2median * a\n",
    "    \n",
    "    for k in range(len(org)):\n",
    "        if np.isnan(distance.loc[k, 'distance']):\n",
    "            VSV1.iloc[k, :] = np.nan\n",
    "        else:\n",
    "            if boundClass1 is not None:\n",
    "                if distance.loc[k, 'label'] == org['REF'].unique()[0]:\n",
    "                    if distance.loc[k, 'distance'] > boundClass1:\n",
    "                        VSV1.iloc[k, :] = np.nan\n",
    "            else:\n",
    "                if boundClass2 is not None:\n",
    "                    if distance.loc[k, 'label'] == org['REF'].unique()[1]:\n",
    "                        if distance.loc[k, 'distance'] > boundClass2:\n",
    "                            VSV1.iloc[k, :] = np.nan\n",
    "    return VSV1\n",
    "\n",
    "# Kernel distance between two points lying in the hyperspace\n",
    "def kern_dis(a, b, kernelfunc):\n",
    "    a = np.array(a).flatten()\n",
    "    b = np.array(b).flatten()\n",
    "    dk = np.sqrt(kernelfunc(a, a) + kernelfunc(b, b) - 2 * kernelfunc(a, b))\n",
    "    return dk\n",
    "\n",
    "# Evaluate the distance between Virtual Support Vectors and Support Vectors lying in the hyperspace\n",
    "def rem_extrem_kerneldist(org, VSV1, a, kernelfunc):\n",
    "    distance = pd.DataFrame(index=range(len(org)), columns=['label', 'distance'])\n",
    "    distanceSVC1 = []\n",
    "    distanceSVC2 = []\n",
    "    \n",
    "    for l in range(len(org)):\n",
    "        distance.loc[l, 'label'] = str(org.iloc[l, -1])\n",
    "        distance.loc[l, 'distance'] = kern_dis(org.iloc[l, :-1], VSV1.iloc[l, :-1], kernelfunc)\n",
    "    \n",
    "    SVClass1 = org[org['REF'] == org['REF'].unique()[0]]\n",
    "    SVClass2 = org[org['REF'] == org['REF'].unique()[1]]\n",
    "    \n",
    "    if len(SVClass1) > 0:\n",
    "        for n in range(len(SVClass1) - 1):\n",
    "            for nn in range(n, len(SVClass1) - 1):\n",
    "                distanceSVC1.append(kern_dis(SVClass1.iloc[n, :-1], SVClass1.iloc[n + nn, :-1], kernelfunc))\n",
    "        disClass1median = np.mean(distanceSVC1)\n",
    "        boundClass1 = disClass1median * a\n",
    "    \n",
    "    if len(SVClass2) > 0:\n",
    "        for n in range(len(SVClass2) - 1):\n",
    "            for nn in range(n, len(SVClass2) - 1):\n",
    "                distanceSVC2.append(kern_dis(SVClass2.iloc[n, :-1], SVClass2.iloc[n + nn, :-1], kernelfunc))\n",
    "        disClass2median = np.mean(distanceSVC2)\n",
    "        boundClass2 = disClass2median * a\n",
    "    \n",
    "    for k in range(len(org)):\n",
    "        if np.isnan(distance.loc[k, 'distance']):\n",
    "            VSV1.iloc[k, :] = np.nan\n",
    "        else:\n",
    "            if boundClass1 is not None:\n",
    "                if distance.loc[k, 'label'] == org['REF'].unique()[0]:\n",
    "                    if distance.loc[k, 'distance'] > boundClass1:\n",
    "                        VSV1.iloc[k, :] = np.nan\n",
    "            else:\n",
    "                if boundClass2 is not None:\n",
    "                    if distance.loc[k, 'label'] == org['REF'].unique()[1]:\n",
    "                        if distance.loc[k, 'distance'] > boundClass2:\n",
    "                            VSV1.iloc[k, :] = np.nan\n",
    "    return VSV1\n",
    "\n",
    "def pred_one(model, data_point):\n",
    "    # Extract necessary components from the SVM model\n",
    "    support_vectors = model.n_support_\n",
    "    kernel_function = model.kernel\n",
    "    coefficients = model.dual_coef_.ravel()\n",
    "    intercept = model.intercept_\n",
    "    \n",
    "    # Initialize prediction variable\n",
    "    prediction = 0\n",
    "    \n",
    "    # Iterate over each support vector\n",
    "    for j in range(len(support_vectors)):\n",
    "        # Compute kernel function value between the j-th support vector and the data point\n",
    "        kernel_value = kernel_function(data_point.reshape(1, -1), model.support_vectors_[j, :].reshape(1, -1))\n",
    "        \n",
    "        # Multiply kernel value by the corresponding coefficient and add to prediction\n",
    "        weighted_value = kernel_value * coefficients[j]\n",
    "        prediction += weighted_value\n",
    "    \n",
    "    # Subtract intercept to get the final prediction\n",
    "    final_prediction = prediction - intercept\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "def uncertainty_dist_v2_2(org, samp):\n",
    "    distance = pd.DataFrame(columns=['control_label', 'distance'], index=range(len(samp)))\n",
    "    \n",
    "    for k in range(len(samp)):\n",
    "        distance.loc[k, 'distance'] = np.sign(pred_one(org.finalModel, samp.iloc[k, :-1])) * \\\n",
    "                                      np.where(pred_one(org.finalModel, samp.iloc[k, :-1]) > 0, 1, -1)\n",
    "    \n",
    "    # Normalize distance\n",
    "    preProc = preprocessing.MinMaxScaler()\n",
    "    preProc.fit(distance[['distance']])\n",
    "    normdistance = preProc.transform(distance[['distance']])\n",
    "    \n",
    "    samp['normdistance'] = normdistance\n",
    "    \n",
    "    return samp\n",
    "\n",
    "def alter_labels(distance_data, ref):\n",
    "    # Merge features and original labels\n",
    "    ref_added = pd.concat([distance_data, ref], axis=1)\n",
    "    # Order by most uncertain samples\n",
    "    ref_added_or = ref_added.sort_values(by='distance')\n",
    "    # Re-label most uncertain n number of samples\n",
    "    ref_added_or.iloc[:250, -1] = ref_added_or.iloc[:250, -2]\n",
    "    ref_added_or.iloc[:250, -2] = 1.0\n",
    "    # Re-order dataset by its index\n",
    "    ref_added_or['index'] = range(len(ref_added_or))\n",
    "    ref_added_reor = ref_added_or.sort_values(by='index')\n",
    "    \n",
    "    # Extract labels for prediction\n",
    "    labels = ref_added_reor.iloc[:, -5]\n",
    "    return labels\n",
    "\n",
    "def ExCsvMSD(datadase, filename=None):\n",
    "    # Convert to numpy array\n",
    "    datadase = np.array(datadase)\n",
    "    n = datadase.shape[1]\n",
    "    MSDdata = np.empty((2, n), dtype=float)\n",
    "    \n",
    "    MSDdata[0, :] = np.mean(datadase, axis=0)\n",
    "    MSDdata[1, :] = np.std(datadase, axis=0)\n",
    "    \n",
    "    MSDdata_final = np.vstack((datadase, MSDdata))\n",
    "    \n",
    "    # Export final mean and standard deviation to .csv-file\n",
    "    if filename is not None:\n",
    "        pd.DataFrame(MSDdata_final).to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    return MSDdata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"cologne_res_100_L2-L13.csv\"\n",
    "sMax = 1000\n",
    "bound = [0.3, 0.6, 0.9]\n",
    "boundMargin = [1.5, 1.0, 0.5]\n",
    "sampleSizesPor = [40, 25, 16, 12, 10, 8, 6, 4, 3, 2, 1]\n",
    "colheader = [\"40\", \"25\", \"16\", \"12\", \"10\", \"8\", \"6\", \"4\", \"3\", \"2\", \"1\"]\n",
    "sindexSVMDATA = 36\n",
    "numFeat = 18\n",
    "eindexSVMDATA = sindexSVMDATA + numFeat - 1\n",
    "objInfoNames = [\"Lx_g_comp\", \"Lx_g_elfi\", \"Lx_g_refi\", \"Lx_g_roun\", \"Lx_g_shin\",\n",
    "                \"Lx_m_bl\", \"Lx_m_gr\", \"Lx_m_ndvi\", \"Lx_m_nir\", \"Lx_m_re\",\n",
    "                \"Lx_sd_bl\", \"Lx_sd_gr\", \"Lx_sd_ndvi\", \"Lx_sd_nir\", \"Lx_sd_re\",\n",
    "                \"Lx_t_diss\", \"Lx_t_hom\", \"Lx_t_mean\",\n",
    "                \"label\"]\n",
    "columnClass = [None] * 217 + [\"factor\", \"integer\"]\n",
    "\n",
    "# Import data\n",
    "preproc_DataPool = pd.read_csv(inputPath, header=0, sep=\";\", dtype=str, na_values=None)\n",
    "\n",
    "tmp_DataPool = preproc_DataPool.iloc[:, :-2]\n",
    "\n",
    "generalDataPool_columns = tmp_DataPool.columns\n",
    "\n",
    "converters = {col: lambda x: float(x.replace(',', '.')) for col in generalDataPool_columns}\n",
    "generalDataPool = pd.read_csv(inputPath, header=0, sep=\";\", na_values=None, converters=converters)\n",
    "\n",
    "\n",
    "generalDataPool.dropna(subset=[\"REF\"], inplace=True)  # Remove rows with missing REF values\n",
    "generalDataPool[\"REF\"] = pd.Categorical(generalDataPool[\"REF\"])\n",
    "\n",
    "# Transform to 2-Class-Case \"Bushes Trees\" VS rest\n",
    "first_label_class = generalDataPool[\"REF\"].cat.categories[0]  # Note that the first record is of class \"bushes trees\"\n",
    "generalDataPool[\"REF\"] = generalDataPool[\"REF\"].apply(lambda x: first_label_class if x == first_label_class else \"other\")\n",
    "generalDataPool[\"REF\"] = pd.Categorical(generalDataPool[\"REF\"])\n",
    "\n",
    "data = generalDataPool.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "REF = generalDataPool.iloc[:, -1]\n",
    "data_with_label = pd.concat([data, REF], axis=1)\n",
    "data_label = data_with_label.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L02_G_COMP     float64\n",
       "L02_G_EFIT     float64\n",
       "L02_G_RFIT     float64\n",
       "L02_G_ROUN     float64\n",
       "L02_G_SHIN     float64\n",
       "                ...   \n",
       "L13_T_DISS     float64\n",
       "L13_T_HOM      float64\n",
       "L13_T_MEA      float64\n",
       "REF           category\n",
       "USE              int64\n",
       "Length: 218, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalDataPool.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizedFeat['L02_G_COMP'] = normalizedFeat['L02_G_COMP'].replace(',', '.', regex=True)\n",
    "\n",
    "#normalizedFeat['L02_G_COMP'] = normalizedFeat['L02_G_COMP'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizedFeat['L02_G_EFIT'] = normalizedFeat['L02_G_EFIT'].apply(lambda x: pd.to_numeric(x.str.replace(',', '.'), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedFeat = generalDataPool.iloc[:, :-2]\n",
    "normalizedLabelUSE = generalDataPool.iloc[:, -2:]\n",
    "\n",
    "# Scaling\n",
    "preProc = MinMaxScaler()\n",
    "normalizedFeatBase = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]), columns=objInfoNames[:-1])\n",
    "\n",
    "# Apply range of basemodel to all levels\n",
    "normalizedFeat2 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, :numFeat]), columns=objInfoNames[:-1])\n",
    "normalizedFeat3 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, numFeat:(2 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat5 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (3 * numFeat):(4 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat6 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (4 * numFeat):(5 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat7 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (5 * numFeat):(6 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat8 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (6 * numFeat):(7 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat9 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (7 * numFeat):(8 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat10 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (8 * numFeat):(9 * numFeat)]), columns=objInfoNames[:-1])\n",
    "normalizedFeat11 = pd.DataFrame(preProc.fit_transform(normalizedFeat.iloc[:, (9 * numFeat):(10 * numFeat)]), columns=objInfoNames[:-1])\n",
    "\n",
    "# Recombine normalized sets to one data frame\n",
    "normalizedDataPoolAllLev = pd.concat([normalizedFeat2, normalizedFeat3, normalizedFeatBase, normalizedFeat5,\n",
    "                                      normalizedFeat6, normalizedFeat7, normalizedFeat8, normalizedFeat9,\n",
    "                                      normalizedFeat10, normalizedFeat11, normalizedLabelUSE], axis=1)\n",
    "\n",
    "# Remove used temporary variables\n",
    "del normalizedFeat, normalizedFeat2, normalizedFeat3, normalizedFeatBase, normalizedFeat5, normalizedFeat6\n",
    "del normalizedFeat7, normalizedFeat8, normalizedFeat9, normalizedFeat10, normalizedFeat11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test, train, and validate data\n",
    "trainDataPoolAllLev, testDataAllLev, validateDataAllLev = [df for _, df in normalizedDataPoolAllLev.groupby('USE')]\n",
    "trainDataPoolAllLev = trainDataPoolAllLev.iloc[:, :-1]\n",
    "testDataAllLev = testDataAllLev.iloc[:, :-1]\n",
    "validateFeatAllLev = validateDataAllLev.iloc[:, :-2]\n",
    "validateLabels = validateDataAllLev.iloc[:, -2]\n",
    "\n",
    "# Order train data pool by class label in alphabetical order\n",
    "trainDataPoolAllLev = trainDataPoolAllLev.sort_values(by=trainDataPoolAllLev.columns[-1])\n",
    "\n",
    "# Current training data-set, updated (refreshed) after each iteration\n",
    "trainDataCur = trainDataPoolAllLev.copy()\n",
    "testDataCur = testDataAllLev.copy()\n",
    "\n",
    "# Set randomized seed for the random sampling procedure\n",
    "seed = 5\n",
    "\n",
    "# Initial seed value for randomized sampling\n",
    "seed += np.random.randint(1, 101)\n",
    "\n",
    "# Definition of apriori-probabilities\n",
    "pA = pB = pC = pD = pE = pF = 1 / 6\n",
    "\n",
    "# Definition of training sample set sizes S [% of max. sample size]\n",
    "sCur = sMax * (sampleSizesPor[0] / 100)\n",
    "# Definition of sample shares\n",
    "nA, nB, nC, nD, nE, nF = [round(sCur * p) for p in [pA, pB, pC, pD, pE, pF]]\n",
    "shares = np.array([nA, nB, nC, nD, nE, nF])\n",
    "\n",
    "# Set randomized seed for the random sampling procedure\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REF'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validateDataAllLev.columns[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp = trainDataCur.groupby('REF', observed=False)[trainDataCur.columns].apply(lambda x: x.sample(67, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42210/1510426551.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stratSamp = trainDataCur.groupby('REF', observed=False).apply(sample_within_group)\n"
     ]
    }
   ],
   "source": [
    "# Define the sampling function\n",
    "def sample_within_group(group):\n",
    "    # Add the original IDs as a new column\n",
    "    group['ID_unit'] = group.index\n",
    "    \n",
    "    # Perform sampling within the group\n",
    "    sampled_group = group.sample(min(len(group), 67), replace=False)\n",
    "    \n",
    "    return sampled_group\n",
    "\n",
    "# Apply the sampling function to each group\n",
    "stratSamp = trainDataCur.groupby('REF', observed=False).apply(sample_within_group)\n",
    "\n",
    "# Reset the index to obtain a flat DataFrame with the original IDs preserved\n",
    "stratSamp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get samples of trainDataCur and set trainDataCur new\n",
    "trainDataCurRemaining = trainDataCur.drop(stratSamp[\"ID_unit\"])\n",
    "\n",
    "# Split test feat from test label for later join with trainData\n",
    "trainFeat = stratSamp.iloc[:, :len(trainDataPoolAllLev.columns)-1]\n",
    "trainLabels = stratSamp.iloc[:, len(trainDataPoolAllLev.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataPoolAllLev.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lx_g_comp', 'Lx_g_elfi', 'Lx_g_refi', 'Lx_g_roun', 'Lx_g_shin',\n",
       "       'Lx_m_bl', 'Lx_m_gr', 'Lx_m_ndvi', 'Lx_m_nir', 'Lx_m_re',\n",
       "       ...\n",
       "       'Lx_m_nir', 'Lx_m_re', 'Lx_sd_bl', 'Lx_sd_gr', 'Lx_sd_ndvi',\n",
       "       'Lx_sd_nir', 'Lx_sd_re', 'Lx_t_diss', 'Lx_t_hom', 'Lx_t_mean'],\n",
       "      dtype='object', length=180)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REF'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratSamp.columns[len(trainDataPoolAllLev.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratSamp = trainDataCur.groupby('REF',observed=False)\n",
    "#stratSamp = stratSamp.apply(lambda x: x.sample(67, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of sampling configuration (strata: random sampling without replacement)\n",
    "#stratSamp = trainDataCur.groupby('REF').apply(lambda x: x.sample(shares, replace=False)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lx_t_diss'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataCur.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      141133\n",
       "1      311080\n",
       "2       27290\n",
       "3       25174\n",
       "4        5752\n",
       "        ...  \n",
       "129    199495\n",
       "130    348052\n",
       "131     63271\n",
       "132    104838\n",
       "133     45255\n",
       "Name: ID_unit, Length: 134, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratSamp.iloc[:,181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      141133\n",
       "1      311080\n",
       "2       27290\n",
       "3       25174\n",
       "4        5752\n",
       "        ...  \n",
       "129    199495\n",
       "130    348052\n",
       "131     63271\n",
       "132    104838\n",
       "133     45255\n",
       "Name: ID_unit, Length: 134, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratSamp[\"ID_unit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeat.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42210/3587032453.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stratSamp = testDataCur.groupby('REF', observed=False).apply(sample_within_group)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subset for each outer iteration test data to speed up computing\n",
    "testDataCur = testDataCur.sort_values(by=testDataCur.columns[-1])\n",
    "# Apply the sampling function to each group\n",
    "stratSamp = testDataCur.groupby('REF', observed=False).apply(sample_within_group)\n",
    "\n",
    "# Split test feat from test label for later join with trainData\n",
    "testFeat = stratSamp.iloc[:, :len(testDataCur.columns)-1]\n",
    "testLabels = stratSamp.iloc[:, len(testDataCur.columns)-1]\n",
    "\n",
    "# Subset on base level\n",
    "testFeatsub = testFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# TrainData index to split between train and test in svmFit\n",
    "countTrainData = trainFeat.shape[0]\n",
    "indexTrainData = [list(range(1, countTrainData + 1))]\n",
    "\n",
    "# SVM base for invariants\n",
    "\n",
    "# Subset on L_4\n",
    "trainFeat = trainFeat.iloc[:, sindexSVMDATA:eindexSVMDATA + 1]\n",
    "\n",
    "# Join train and test data (separable through indexTrainData in svmFit)\n",
    "tuneFeat = pd.concat([trainFeat, testFeatsub], axis=0)\n",
    "tuneLabel = np.concatenate((trainLabels.values, testLabels.values))\n",
    "\n",
    "validateFeatsub = validateFeatAllLev.iloc[:, sindexSVMDATA:eindexSVMDATA+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         'gamma': array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "# Expand coarse grid\n",
    "coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "\n",
    "\n",
    "# Coarse grid search\n",
    "svm_coarse = SVC(kernel='rbf')\n",
    "svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring='accuracy')#, cv=indexTrainData)\n",
    "svm_coarse_cv.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl  \\\n",
       "0                 0.881864   0.099390   0.033490   0.043741  0.041110   \n",
       "1                 0.772282   0.222935   0.136538   0.036692  0.039930   \n",
       "2                 0.839724   0.136943   0.070390   0.035988  0.034338   \n",
       "3                 0.703653   0.346674   0.221069   0.037933  0.033814   \n",
       "4                 0.763603   0.296652   0.222943   0.026393  0.019910   \n",
       "...                    ...        ...        ...        ...       ...   \n",
       "(other, 385192)   0.819644   0.159218   0.150555   0.155139  0.206465   \n",
       "(other, 722141)   0.129779   0.623823   0.431590   0.067814  0.070626   \n",
       "(other, 647980)   0.616562   0.336806   0.330787   0.114776  0.126091   \n",
       "(other, 452074)   0.789957   0.383627   0.285932   0.039511  0.033419   \n",
       "(other, 488555)   0.478919   0.686964   0.702629   0.035380  0.030130   \n",
       "\n",
       "                  Lx_m_gr  Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  \\\n",
       "0                0.594956   0.108474  0.048582  0.056026  0.044968  0.284792   \n",
       "1                0.696350   0.123440  0.034328  0.055595  0.055192  0.565725   \n",
       "2                0.611937   0.101712  0.039555  0.054533  0.046203  0.211215   \n",
       "3                0.590421   0.090443  0.035975  0.055647  0.049317  0.171620   \n",
       "4                0.655712   0.084793  0.018825  0.070159  0.052613  0.498458   \n",
       "...                   ...        ...       ...       ...       ...       ...   \n",
       "(other, 385192)  0.375743   0.194059  0.217506  0.115224  0.120812  0.038899   \n",
       "(other, 722141)  0.477957   0.094662  0.066868  0.212782  0.180043  0.647936   \n",
       "(other, 647980)  0.353036   0.107546  0.121120  0.279101  0.260963  0.146093   \n",
       "(other, 452074)  0.324108   0.037349  0.037695  0.038224  0.030883  0.292498   \n",
       "(other, 488555)  0.289344   0.026724  0.029547  0.080979  0.081011  0.332346   \n",
       "\n",
       "                 Lx_sd_ndvi  Lx_sd_nir  Lx_sd_re  Lx_t_diss  Lx_t_hom  \\\n",
       "0                  0.139878   0.043269  0.143899   0.032592  0.777998   \n",
       "1                  0.224939   0.075415  0.133732   0.039599  0.776734   \n",
       "2                  0.115927   0.056797  0.146390   0.035231  0.794930   \n",
       "3                  0.105939   0.052385  0.182598   0.027399  0.767423   \n",
       "4                  0.220374   0.044596  0.120890   0.046761  0.794327   \n",
       "...                     ...        ...       ...        ...       ...   \n",
       "(other, 385192)    0.127993   0.115898  0.280566   0.014351  0.772224   \n",
       "(other, 722141)    0.247446   0.186520  0.229887   0.028220  0.799774   \n",
       "(other, 647980)    0.246706   0.242621  0.268329   0.016238  0.743494   \n",
       "(other, 452074)    0.073664   0.044345  0.176602   0.036870  0.831269   \n",
       "(other, 488555)    0.111932   0.079218  0.192559   0.027573  0.819508   \n",
       "\n",
       "                 Lx_t_mean  Lx_g_comp  \n",
       "0                 0.035762   0.035762  \n",
       "1                 0.086452   0.086452  \n",
       "2                 0.079855   0.079855  \n",
       "3                 0.130563   0.130563  \n",
       "4                 0.173672   0.173672  \n",
       "...                    ...        ...  \n",
       "(other, 385192)   0.097817   0.097817  \n",
       "(other, 722141)   0.362326   0.362326  \n",
       "(other, 647980)   0.185262   0.185262  \n",
       "(other, 452074)   0.249907   0.249907  \n",
       "(other, 488555)   0.346446   0.346446  \n",
       "\n",
       "[268 rows x 18 columns]>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuneFeat.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'bushes_trees', 'bushes_trees', 'bushes_trees', 'bushes_trees',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other', 'other', 'other', 'other',\n",
       "       'other', 'other', 'other', 'other'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuneLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         &#x27;gamma&#x27;: array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([6.250e-02, 2.500e-01, 1.000e+00, 4.000e+00, 1.600e+01, 6.400e+01,\n",
       "       2.560e+02, 1.024e+03, 4.096e+03]),\n",
       "                         'gamma': array([0.03125, 0.125  , 0.5    , 2.     , 8.     ])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand coarse grid\n",
    "coarse_grid = {'C': 2.0 ** np.arange(-4, 13, 2),\n",
    "                'gamma': 2.0 ** np.arange(-5, 4, 2)}\n",
    "\n",
    "# Coarse grid search\n",
    "svm_coarse = SVC(kernel='rbf')\n",
    "svm_coarse_cv = GridSearchCV(svm_coarse, param_grid=coarse_grid, scoring='accuracy')#, cv=indexTrainData)\n",
    "svm_coarse_cv.fit(tuneFeat, tuneLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([ 4.        ,  5.65685425,  8.        , 11.3137085 , 16.        ,\n",
       "       22.627417  , 32.        , 45.254834  ]),\n",
       "                         &#x27;gamma&#x27;: array([0.125     , 0.1767767 , 0.25      , 0.35355339, 0.5       ,\n",
       "       0.70710678, 1.        , 1.41421356])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: array([ 4.        ,  5.65685425,  8.        , 11.3137085 , 16.        ,\n",
       "       22.627417  , 32.        , 45.254834  ]),\n",
       "                         &#x27;gamma&#x27;: array([0.125     , 0.1767767 , 0.25      , 0.35355339, 0.5       ,\n",
       "       0.70710678, 1.        , 1.41421356])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([ 4.        ,  5.65685425,  8.        , 11.3137085 , 16.        ,\n",
       "       22.627417  , 32.        , 45.254834  ]),\n",
       "                         'gamma': array([0.125     , 0.1767767 , 0.25      , 0.35355339, 0.5       ,\n",
       "       0.70710678, 1.        , 1.41421356])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best coarse grid parameters\n",
    "best_c = svm_coarse_cv.best_params_['C']\n",
    "best_gamma = svm_coarse_cv.best_params_['gamma']\n",
    "\n",
    "# Define narrow grid borders\n",
    "a_gamma = np.log2(best_gamma) - 2\n",
    "b_gamma = np.log2(best_gamma) + 2\n",
    "a_c = np.log2(best_c) - 2\n",
    "b_c = np.log2(best_c) + 2\n",
    "\n",
    "# Expand narrow grid\n",
    "narrow_grid = {'C': 2.0 ** np.arange(a_c, b_c, 0.5),\n",
    "                'gamma': 2.0 ** np.arange(a_gamma, b_gamma, 0.5)}\n",
    "\n",
    "# Narrow grid search\n",
    "svm_narrow = SVC(kernel='rbf')\n",
    "svm_narrow_cv = GridSearchCV(svm_narrow, param_grid=narrow_grid, scoring='accuracy')#, cv=indexTrainData)\n",
    "svm_narrow_cv.fit(tuneFeat, tuneLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM parameter tuning\n",
    "svm_model=svm_fit(tuneFeat, tuneLabel, indexTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522473    bushes_trees\n",
       "522474    bushes_trees\n",
       "523472    bushes_trees\n",
       "523473           other\n",
       "524470    bushes_trees\n",
       "              ...     \n",
       "997994           other\n",
       "997995           other\n",
       "997996           other\n",
       "997997           other\n",
       "997998           other\n",
       "Name: REF, Length: 333323, dtype: category\n",
       "Categories (2, object): ['bushes_trees', 'other']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validateLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8594306423499128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict labels of test data\n",
    "predLabelsSVM = svm_model.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accSVM = accuracy_score(validateLabels, predLabelsSVM)\n",
    "print(accSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m indexTrainData \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, countTrainData \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Join of train and test test data (through indexTrainData in svmFit separable)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m tuneFeatVSVM \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrainFeatVSVM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestFeatsub\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m tuneLabelsVSVM \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((trainLabelsVSVM\u001b[38;5;241m.\u001b[39mvalues, testLabels\u001b[38;5;241m.\u001b[39mvalues))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# VSVM parameter tuning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rsrgpy/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rsrgpy/lib/python3.12/site-packages/pandas/core/reshape/concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m \u001b[43mobj_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rsrgpy/lib/python3.12/site-packages/pandas/core/indexes/base.py:3885\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "# VSVM on all Level SV\n",
    "SVindex = svm_model.best_estimator_.support_  # Indices of support vectors\n",
    "SVtotal = trainDataCur.iloc[SVindex, sindexSVMDATA:eindexSVMDATA+1].reset_index(drop=True)  # Get support vectors\n",
    "\n",
    "SVL2 = trainDataCur.iloc[SVindex, sindexSVMDATA - 2 * numFeat:sindexSVMDATA - numFeat ].reset_index(drop=True)\n",
    "SVL3 = trainDataCur.iloc[SVindex, sindexSVMDATA - numFeat:sindexSVMDATA ].reset_index(drop=True)\n",
    "\n",
    "SVL5 = trainDataCur.iloc[SVindex, sindexSVMDATA + numFeat:sindexSVMDATA + 2 * numFeat ].reset_index(drop=True)\n",
    "SVL6 = trainDataCur.iloc[SVindex, sindexSVMDATA + 2 * numFeat:sindexSVMDATA + 3 * numFeat ].reset_index(drop=True)\n",
    "SVL7 = trainDataCur.iloc[SVindex, sindexSVMDATA + 3 * numFeat:sindexSVMDATA + 4 * numFeat ].reset_index(drop=True)\n",
    "SVL8 = trainDataCur.iloc[SVindex, sindexSVMDATA + 4 * numFeat:sindexSVMDATA + 5 * numFeat ].reset_index(drop=True)\n",
    "SVL9 = trainDataCur.iloc[SVindex, sindexSVMDATA + 5 * numFeat:sindexSVMDATA + 6 * numFeat ].reset_index(drop=True)\n",
    "SVL10 = trainDataCur.iloc[SVindex, sindexSVMDATA + 6 * numFeat:sindexSVMDATA + 7 * numFeat ].reset_index(drop=True)\n",
    "SVL11 = trainDataCur.iloc[SVindex, sindexSVMDATA + 7 * numFeat:sindexSVMDATA + 8 * numFeat ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Bind original SV with modified to new train data set\n",
    "SVinvar = pd.concat([SVtotal, SVL2, SVL3, SVL5, SVL6, SVL7, SVL8, SVL9, SVL10, SVL11],ignore_index=True)\n",
    "\n",
    "# Split for training to feature and label\n",
    "trainFeatVSVM = SVinvar.iloc[:, :-1]\n",
    "trainLabelsVSVM = SVinvar.iloc[:, -1]\n",
    "\n",
    "# Get list with index of train data to split between train and test in svmFit\n",
    "countTrainData = SVinvar.shape[0]\n",
    "indexTrainData = [list(range(1, countTrainData + 1))]\n",
    "\n",
    "# Join of train and test test data (through indexTrainData in svmFit separable)\n",
    "tuneFeatVSVM = pd.concat([trainFeatVSVM, testFeatsub], axis=0)\n",
    "tuneLabelsVSVM = np.concatenate((trainLabelsVSVM.values, testLabels.values))\n",
    "\n",
    "# VSVM parameter tuning\n",
    "tunedVSVM = svm_fit(tuneFeatVSVM, tuneLabelsVSVM)\n",
    "\n",
    "# Run classification and accuracy assessment for modified SV\n",
    "predLabelsVSVM = tunedVSVM.predict(validateFeatsub)\n",
    "accVSVM = accuracy_score(validateLabels, predLabelsVSVM)\n",
    "print(accVSVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.785337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.785337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "      <td>0.803718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052193</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.142156</td>\n",
       "      <td>0.049890</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>0.678320</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>0.091693</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.213381</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.770114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.052193</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.142156</td>\n",
       "      <td>0.049890</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>0.678320</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>0.091693</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.213381</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.770114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.743396</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>0.147286</td>\n",
       "      <td>0.074513</td>\n",
       "      <td>0.037590</td>\n",
       "      <td>0.032785</td>\n",
       "      <td>0.566912</td>\n",
       "      <td>0.081766</td>\n",
       "      <td>0.034202</td>\n",
       "      <td>0.053797</td>\n",
       "      <td>0.046509</td>\n",
       "      <td>0.229015</td>\n",
       "      <td>0.105233</td>\n",
       "      <td>0.037623</td>\n",
       "      <td>0.174866</td>\n",
       "      <td>0.026871</td>\n",
       "      <td>0.783304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.743396</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>0.147286</td>\n",
       "      <td>0.074513</td>\n",
       "      <td>0.037590</td>\n",
       "      <td>0.032785</td>\n",
       "      <td>0.566912</td>\n",
       "      <td>0.081766</td>\n",
       "      <td>0.034202</td>\n",
       "      <td>0.053797</td>\n",
       "      <td>0.046509</td>\n",
       "      <td>0.229015</td>\n",
       "      <td>0.105233</td>\n",
       "      <td>0.037623</td>\n",
       "      <td>0.174866</td>\n",
       "      <td>0.026871</td>\n",
       "      <td>0.783304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.067140</td>\n",
       "      <td>0.759791</td>\n",
       "      <td>0.842358</td>\n",
       "      <td>0.139249</td>\n",
       "      <td>0.106003</td>\n",
       "      <td>0.032682</td>\n",
       "      <td>0.034175</td>\n",
       "      <td>0.707802</td>\n",
       "      <td>0.126718</td>\n",
       "      <td>0.033567</td>\n",
       "      <td>0.053523</td>\n",
       "      <td>0.054488</td>\n",
       "      <td>0.388945</td>\n",
       "      <td>0.165385</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.060635</td>\n",
       "      <td>0.789355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl   Lx_m_gr  \\\n",
       "0   0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "1   0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "2   0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "3   0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "4   0.073022   0.670330   0.798266   0.201985   0.077653  0.021294  0.014574   \n",
       "5   0.052193   0.707317   0.819672   0.142156   0.049890  0.033915  0.035445   \n",
       "6   0.052193   0.707317   0.819672   0.142156   0.049890  0.033915  0.035445   \n",
       "7   0.063844   0.743396   0.851452   0.147286   0.074513  0.037590  0.032785   \n",
       "8   0.063844   0.743396   0.851452   0.147286   0.074513  0.037590  0.032785   \n",
       "9   0.067140   0.759791   0.842358   0.139249   0.106003  0.032682  0.034175   \n",
       "\n",
       "   Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  \\\n",
       "0   0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "1   0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "2   0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "3   0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "4   0.631935  0.061670  0.007359  0.026509  0.022434    0.316412   0.119451   \n",
       "5   0.678320  0.109000  0.029649  0.026247  0.020902    0.091693   0.051384   \n",
       "6   0.678320  0.109000  0.029649  0.026247  0.020902    0.091693   0.051384   \n",
       "7   0.566912  0.081766  0.034202  0.053797  0.046509    0.229015   0.105233   \n",
       "8   0.566912  0.081766  0.034202  0.053797  0.046509    0.229015   0.105233   \n",
       "9   0.707802  0.126718  0.033567  0.053523  0.054488    0.388945   0.165385   \n",
       "\n",
       "   Lx_sd_re  Lx_t_diss  Lx_t_hom  Lx_t_mean  \n",
       "0  0.030528   0.124826  0.042560   0.785337  \n",
       "1  0.030528   0.124826  0.042560   0.785337  \n",
       "2  0.159947   0.146974  0.050292   0.826420  \n",
       "3  0.159947   0.146974  0.050292   0.826420  \n",
       "4  0.020073   0.098026  0.060738   0.803718  \n",
       "5  0.018004   0.213381  0.016879   0.770114  \n",
       "6  0.018004   0.213381  0.016879   0.770114  \n",
       "7  0.037623   0.174866  0.026871   0.783304  \n",
       "8  0.037623   0.174866  0.026871   0.783304  \n",
       "9  0.083500   0.099998  0.060635   0.789355  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVtotal[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081117</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.831181</td>\n",
       "      <td>0.131844</td>\n",
       "      <td>0.076223</td>\n",
       "      <td>0.034446</td>\n",
       "      <td>0.038443</td>\n",
       "      <td>0.783527</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>0.042728</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.203268</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>0.049257</td>\n",
       "      <td>0.083217</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.683222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081117</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.831181</td>\n",
       "      <td>0.131844</td>\n",
       "      <td>0.076223</td>\n",
       "      <td>0.034446</td>\n",
       "      <td>0.038443</td>\n",
       "      <td>0.783527</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>0.042728</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.203268</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>0.049257</td>\n",
       "      <td>0.083217</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.683222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085631</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.737209</td>\n",
       "      <td>0.248516</td>\n",
       "      <td>0.078160</td>\n",
       "      <td>0.030705</td>\n",
       "      <td>0.045201</td>\n",
       "      <td>0.649178</td>\n",
       "      <td>0.120411</td>\n",
       "      <td>0.058535</td>\n",
       "      <td>0.060847</td>\n",
       "      <td>0.119105</td>\n",
       "      <td>0.300510</td>\n",
       "      <td>0.184781</td>\n",
       "      <td>0.180655</td>\n",
       "      <td>0.142330</td>\n",
       "      <td>0.023999</td>\n",
       "      <td>0.726856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.715810</td>\n",
       "      <td>0.129485</td>\n",
       "      <td>0.162943</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.653787</td>\n",
       "      <td>0.088553</td>\n",
       "      <td>0.036543</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.046669</td>\n",
       "      <td>0.140672</td>\n",
       "      <td>0.093051</td>\n",
       "      <td>0.057494</td>\n",
       "      <td>0.176797</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.748122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.292604</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.477876</td>\n",
       "      <td>0.597481</td>\n",
       "      <td>0.375123</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.031084</td>\n",
       "      <td>0.709437</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.031764</td>\n",
       "      <td>0.051962</td>\n",
       "      <td>0.038266</td>\n",
       "      <td>0.127531</td>\n",
       "      <td>0.114355</td>\n",
       "      <td>0.028668</td>\n",
       "      <td>0.178123</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.702376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.084326</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.784747</td>\n",
       "      <td>0.192395</td>\n",
       "      <td>0.078921</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.043656</td>\n",
       "      <td>0.686006</td>\n",
       "      <td>0.114641</td>\n",
       "      <td>0.046953</td>\n",
       "      <td>0.039727</td>\n",
       "      <td>0.033532</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>0.074616</td>\n",
       "      <td>0.029051</td>\n",
       "      <td>0.142254</td>\n",
       "      <td>0.016950</td>\n",
       "      <td>0.669979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.084326</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.784747</td>\n",
       "      <td>0.192395</td>\n",
       "      <td>0.078921</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.043656</td>\n",
       "      <td>0.686006</td>\n",
       "      <td>0.114641</td>\n",
       "      <td>0.046953</td>\n",
       "      <td>0.039727</td>\n",
       "      <td>0.033532</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>0.074616</td>\n",
       "      <td>0.029051</td>\n",
       "      <td>0.142254</td>\n",
       "      <td>0.016950</td>\n",
       "      <td>0.669979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.103148</td>\n",
       "      <td>0.743396</td>\n",
       "      <td>0.822682</td>\n",
       "      <td>0.199338</td>\n",
       "      <td>0.117872</td>\n",
       "      <td>0.042047</td>\n",
       "      <td>0.041488</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.051128</td>\n",
       "      <td>0.081426</td>\n",
       "      <td>0.074613</td>\n",
       "      <td>0.186093</td>\n",
       "      <td>0.152812</td>\n",
       "      <td>0.060707</td>\n",
       "      <td>0.116578</td>\n",
       "      <td>0.026941</td>\n",
       "      <td>0.681453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.103148</td>\n",
       "      <td>0.743396</td>\n",
       "      <td>0.822682</td>\n",
       "      <td>0.199338</td>\n",
       "      <td>0.117872</td>\n",
       "      <td>0.042047</td>\n",
       "      <td>0.041488</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.051128</td>\n",
       "      <td>0.081426</td>\n",
       "      <td>0.074613</td>\n",
       "      <td>0.186093</td>\n",
       "      <td>0.152812</td>\n",
       "      <td>0.060707</td>\n",
       "      <td>0.116578</td>\n",
       "      <td>0.026941</td>\n",
       "      <td>0.681453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.090983</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.750619</td>\n",
       "      <td>0.191248</td>\n",
       "      <td>0.126410</td>\n",
       "      <td>0.035457</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.815578</td>\n",
       "      <td>0.158897</td>\n",
       "      <td>0.041787</td>\n",
       "      <td>0.042882</td>\n",
       "      <td>0.032771</td>\n",
       "      <td>0.107264</td>\n",
       "      <td>0.099906</td>\n",
       "      <td>0.049345</td>\n",
       "      <td>0.131846</td>\n",
       "      <td>0.039565</td>\n",
       "      <td>0.664842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl   Lx_m_gr  \\\n",
       "0   0.081117   0.831325   0.831181   0.131844   0.076223  0.034446  0.038443   \n",
       "1   0.081117   0.831325   0.831181   0.131844   0.076223  0.034446  0.038443   \n",
       "2   0.085631   0.647059   0.737209   0.248516   0.078160  0.030705  0.045201   \n",
       "3   0.149065   0.666667   0.715810   0.129485   0.162943  0.027486  0.031470   \n",
       "4   0.292604   0.037037   0.477876   0.597481   0.375123  0.030327  0.031084   \n",
       "5   0.084326   0.707317   0.784747   0.192395   0.078921  0.038859  0.043656   \n",
       "6   0.084326   0.707317   0.784747   0.192395   0.078921  0.038859  0.043656   \n",
       "7   0.103148   0.743396   0.822682   0.199338   0.117872  0.042047  0.041488   \n",
       "8   0.103148   0.743396   0.822682   0.199338   0.117872  0.042047  0.041488   \n",
       "9   0.090983   0.600000   0.750619   0.191248   0.126410  0.035457  0.043464   \n",
       "\n",
       "   Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  \\\n",
       "0   0.783527  0.140820  0.040684  0.042728  0.037975    0.203268   0.188933   \n",
       "1   0.783527  0.140820  0.040684  0.042728  0.037975    0.203268   0.188933   \n",
       "2   0.649178  0.120411  0.058535  0.060847  0.119105    0.300510   0.184781   \n",
       "3   0.653787  0.088553  0.036543  0.058980  0.046669    0.140672   0.093051   \n",
       "4   0.709437  0.096326  0.031764  0.051962  0.038266    0.127531   0.114355   \n",
       "5   0.686006  0.114641  0.046953  0.039727  0.033532    0.074507   0.074616   \n",
       "6   0.686006  0.114641  0.046953  0.039727  0.033532    0.074507   0.074616   \n",
       "7   0.577260  0.089192  0.051128  0.081426  0.074613    0.186093   0.152812   \n",
       "8   0.577260  0.089192  0.051128  0.081426  0.074613    0.186093   0.152812   \n",
       "9   0.815578  0.158897  0.041787  0.042882  0.032771    0.107264   0.099906   \n",
       "\n",
       "   Lx_sd_re  Lx_t_diss  Lx_t_hom  Lx_t_mean  \n",
       "0  0.049257   0.083217  0.042629   0.683222  \n",
       "1  0.049257   0.083217  0.042629   0.683222  \n",
       "2  0.180655   0.142330  0.023999   0.726856  \n",
       "3  0.057494   0.176797  0.003772   0.748122  \n",
       "4  0.028668   0.178123  0.015826   0.702376  \n",
       "5  0.029051   0.142254  0.016950   0.669979  \n",
       "6  0.029051   0.142254  0.016950   0.669979  \n",
       "7  0.060707   0.116578  0.026941   0.681453  \n",
       "8  0.060707   0.116578  0.026941   0.681453  \n",
       "9  0.049345   0.131846  0.039565   0.664842  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVL2[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lx_g_comp</th>\n",
       "      <th>Lx_g_elfi</th>\n",
       "      <th>Lx_g_refi</th>\n",
       "      <th>Lx_g_roun</th>\n",
       "      <th>Lx_g_shin</th>\n",
       "      <th>Lx_m_bl</th>\n",
       "      <th>Lx_m_gr</th>\n",
       "      <th>Lx_m_ndvi</th>\n",
       "      <th>Lx_m_nir</th>\n",
       "      <th>Lx_m_re</th>\n",
       "      <th>Lx_sd_bl</th>\n",
       "      <th>Lx_sd_gr</th>\n",
       "      <th>Lx_sd_ndvi</th>\n",
       "      <th>Lx_sd_nir</th>\n",
       "      <th>Lx_sd_re</th>\n",
       "      <th>Lx_t_diss</th>\n",
       "      <th>Lx_t_hom</th>\n",
       "      <th>Lx_t_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.785337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050208</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.048185</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.778228</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.250152</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.124826</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.785337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.273995</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.658483</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.108569</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>0.336826</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.826420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.798266</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.316412</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.060738</td>\n",
       "      <td>0.803718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.128719</td>\n",
       "      <td>0.450644</td>\n",
       "      <td>0.696731</td>\n",
       "      <td>0.415129</td>\n",
       "      <td>0.209054</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.046634</td>\n",
       "      <td>0.732685</td>\n",
       "      <td>0.160815</td>\n",
       "      <td>0.046254</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.036228</td>\n",
       "      <td>0.225737</td>\n",
       "      <td>0.110220</td>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.192499</td>\n",
       "      <td>0.038910</td>\n",
       "      <td>0.788608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.893459</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.130733</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.807497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.893459</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.130733</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.807497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.893459</td>\n",
       "      <td>0.113375</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.037481</td>\n",
       "      <td>0.187865</td>\n",
       "      <td>0.130733</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.807497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.091831</td>\n",
       "      <td>0.693431</td>\n",
       "      <td>0.830558</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.105103</td>\n",
       "      <td>0.025162</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.604674</td>\n",
       "      <td>0.088022</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.041283</td>\n",
       "      <td>0.034824</td>\n",
       "      <td>0.108085</td>\n",
       "      <td>0.078686</td>\n",
       "      <td>0.036023</td>\n",
       "      <td>0.262832</td>\n",
       "      <td>0.028123</td>\n",
       "      <td>0.822724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lx_g_comp  Lx_g_elfi  Lx_g_refi  Lx_g_roun  Lx_g_shin   Lx_m_bl   Lx_m_gr  \\\n",
       "0    0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "1    0.050208   0.831325   0.858572   0.097416   0.048185  0.028829  0.029048   \n",
       "2    0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "3    0.094363   0.594595   0.789575   0.273995   0.186283  0.029562  0.038956   \n",
       "4    0.073022   0.670330   0.798266   0.201985   0.077653  0.021294  0.014574   \n",
       "..        ...        ...        ...        ...        ...       ...       ...   \n",
       "84   0.128719   0.450644   0.696731   0.415129   0.209054  0.033694  0.046634   \n",
       "85   0.048208   0.821429   0.893459   0.113375   0.126157  0.033787  0.043146   \n",
       "86   0.048208   0.821429   0.893459   0.113375   0.126157  0.033787  0.043146   \n",
       "87   0.048208   0.821429   0.893459   0.113375   0.126157  0.033787  0.043146   \n",
       "88   0.091831   0.693431   0.830558   0.193400   0.105103  0.025162  0.027748   \n",
       "\n",
       "    Lx_m_ndvi  Lx_m_nir   Lx_m_re  Lx_sd_bl  Lx_sd_gr  Lx_sd_ndvi  Lx_sd_nir  \\\n",
       "0    0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "1    0.778228  0.137015  0.022815  0.028229  0.023671    0.250152   0.130107   \n",
       "2    0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "3    0.658483  0.126350  0.045296  0.087441  0.108569    0.624496   0.336826   \n",
       "4    0.631935  0.061670  0.007359  0.026509  0.022434    0.316412   0.119451   \n",
       "..        ...       ...       ...       ...       ...         ...        ...   \n",
       "84   0.732685  0.160815  0.046254  0.042091  0.036228    0.225737   0.110220   \n",
       "85   0.646950  0.127958  0.049292  0.047404  0.037481    0.187865   0.130733   \n",
       "86   0.646950  0.127958  0.049292  0.047404  0.037481    0.187865   0.130733   \n",
       "87   0.646950  0.127958  0.049292  0.047404  0.037481    0.187865   0.130733   \n",
       "88   0.604674  0.088022  0.031135  0.041283  0.034824    0.108085   0.078686   \n",
       "\n",
       "    Lx_sd_re  Lx_t_diss  Lx_t_hom  Lx_t_mean  \n",
       "0   0.030528   0.124826  0.042560   0.785337  \n",
       "1   0.030528   0.124826  0.042560   0.785337  \n",
       "2   0.159947   0.146974  0.050292   0.826420  \n",
       "3   0.159947   0.146974  0.050292   0.826420  \n",
       "4   0.020073   0.098026  0.060738   0.803718  \n",
       "..       ...        ...       ...        ...  \n",
       "84  0.048571   0.192499  0.038910   0.788608  \n",
       "85  0.036498   0.197834  0.042960   0.807497  \n",
       "86  0.036498   0.197834  0.042960   0.807497  \n",
       "87  0.036498   0.197834  0.042960   0.807497  \n",
       "88  0.036023   0.262832  0.028123   0.822724  \n",
       "\n",
       "[267 rows x 18 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([SVtotal,SVL2,SVL5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# VSVM - EVALUATION of all Level VSV\n",
    "actKappa = 0\n",
    "bestFittingModel = None\n",
    "\n",
    "# Iteration over bound to test different bound thresholds determining the radius of acceptance\n",
    "for jj, bound_val in enumerate(bound, start=1):\n",
    "    SVinvarRadi_list = []\n",
    "    \n",
    "    # Iterating over boundMargin to test different thresholds on margin distance\n",
    "    for kk, bound_margin_val in enumerate(boundMargin, start=1):\n",
    "        SVinvar_list = []\n",
    "        \n",
    "        # Iterate over SVinvarRadi and evaluate distance to hyperplane\n",
    "        for m in range(len(SVinvarRadi)):\n",
    "            signa = pred_one(tunedSVM.finalModel, SVinvarRadi[m, :-1])\n",
    "            if SVinvarRadi[m, -1] == levels(generalDataPool.REF)[0]:\n",
    "                if -bound_margin_val < signa < bound_margin_val:\n",
    "                    SVinvar_list.append(SVinvarRadi[m, :])\n",
    "            else:\n",
    "                if -bound_margin_val < signa < bound_margin_val:\n",
    "                    SVinvar_list.append(SVinvarRadi[m, :])\n",
    "\n",
    "        SVinvar = pd.DataFrame(SVinvar_list, columns=objInfoNames)\n",
    "        \n",
    "        # Merge elected VSV with original SV\n",
    "        SVinvar_org = pd.concat([SVtotal, SVinvar])\n",
    "\n",
    "        # Split for training to feature and label\n",
    "        trainFeatVSVM = SVinvar_org.iloc[:, :-1]\n",
    "        trainLabelsVSVM = SVinvar_org.iloc[:, -1]\n",
    "\n",
    "        # Get list with index of trainData to split between train and test in svmFit\n",
    "        countTrainData = SVinvar_org.shape[0]\n",
    "        indexTrainData = [list(range(1, countTrainData + 1))]\n",
    "\n",
    "        # Join of train and test data (through indexTrainData in svmFit separable)\n",
    "        names = objInfoNames[:-1]\n",
    "        tuneFeatVSVM = pd.concat([trainFeatVSVM, testFeatsub], axis=0)\n",
    "        tuneFeatVSVM.columns = names\n",
    "        tuneLabelsVSVM = np.concatenate((trainLabelsVSVM.values, testLabels.values))\n",
    "\n",
    "        ######################################## VSVM control parameter tuning ########################################\n",
    "        tunedVSVM = SVC(kernel='linear')\n",
    "        tunedVSVM.fit(tuneFeatVSVM, tuneLabelsVSVM)\n",
    "\n",
    "        # Get the best fitting model based on Kappa\n",
    "        if actKappa < tunedVSVM.resample.Kappa:\n",
    "            bestFittingModel = tunedVSVM\n",
    "            actKappa = tunedVSVM.resample.Kappa\n",
    "\n",
    "# Run classification and accuracy assessment for the best bound setting\n",
    "# Predict labels of test data\n",
    "predLabelsVSVMsum = bestFittingModel.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL = accuracy_score(validateLabels, predLabelsVSVMsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Balanced & Random unlabeled samples\n",
    "# Balanced samples\n",
    "\n",
    "# Definition of sampling configuration (strata: random sampling without replacement)\n",
    "stratSampRemaining_b = resample(trainDataCurRemaining, n_samples=[b, b, b, b, b, b], replace=False)\n",
    "samplesRemaining_b = trainDataCurRemaining.iloc[stratSampRemaining_b]\n",
    "\n",
    "trainDataCurRemaining_b = samplesRemaining_b.iloc[:, :-1]\n",
    "trainDataCurRemainingsub_b = trainDataCurRemaining_b.iloc[:, sindexSVMDATA:eindexSVMDATA]\n",
    "REF_b = bestFittingModel.predict(trainDataCurRemainingsub_b)\n",
    "\n",
    "SVindexUn_b = np.arange(1, len(trainDataCurRemainingsub_b) + 1)\n",
    "SVtotalUn_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA:eindexSVMDATA]\n",
    "SVtotalUn_b['REF'] = REF_b\n",
    "\n",
    "SVL2Un_b = trainDataCurRemaining.iloc[SVindexUn_b - 1, sindexSVMDATA - 2*numFeat:sindexSVMDATA - numFeat - 1].copy()\n",
    "SVL2Un_b['REF'] = REF_b\n",
    "SVL3Un_b = trainDataCurRemaining.iloc[SVindexUn_b - 1, sindexSVMDATA - numFeat:sindexSVMDATA - 1].copy()\n",
    "SVL3Un_b['REF'] = REF_b\n",
    "SVL5Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + numFeat:sindexSVMDATA + 2*numFeat - 1].copy()\n",
    "SVL5Un_b['REF'] = REF_b\n",
    "SVL6Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 2*numFeat:sindexSVMDATA + 3*numFeat - 1].copy()\n",
    "SVL6Un_b['REF'] = REF_b\n",
    "SVL7Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 3*numFeat:sindexSVMDATA + 4*numFeat - 1].copy()\n",
    "SVL7Un_b['REF'] = REF_b\n",
    "SVL8Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 4*numFeat:sindexSVMDATA + 5*numFeat - 1].copy()\n",
    "SVL8Un_b['REF'] = REF_b\n",
    "SVL9Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 5*numFeat:sindexSVMDATA + 6*numFeat - 1].copy()\n",
    "SVL9Un_b['REF'] = REF_b\n",
    "SVL10Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 6*numFeat:sindexSVMDATA + 7*numFeat - 1].copy()\n",
    "SVL10Un_b['REF'] = REF_b\n",
    "SVL11Un_b = trainDataCurRemaining_b.iloc[SVindexUn_b - 1, sindexSVMDATA + 7*numFeat:sindexSVMDATA + 8*numFeat - 1].copy()\n",
    "SVL11Un_b['REF'] = REF_b\n",
    "\n",
    "SVinvarUn_b = pd.concat([SVtotalUn_b, SVL2Un_b, SVL3Un_b, SVL5Un_b, SVL6Un_b, SVL7Un_b, SVL8Un_b, SVL9Un_b, SVL10Un_b, SVL11Un_b])\n",
    "\n",
    "# Balanced Unlabeled samples\n",
    "\n",
    "actKappa = 0\n",
    "\n",
    "for jj in range(len(bound)):\n",
    "    SVinvarRadiUn_b_list = []\n",
    "    for m in range(len(SVinvarRadiUn_b)):\n",
    "        signa = pred_one(tunedSVM.finalModel, SVinvarRadiUn_b.iloc[m, :-1])\n",
    "        if SVinvarRadiUn_b.iloc[m, -1] == levels(generalDataPool.REF)[0]:\n",
    "            if -bound_margin_val < signa < bound_margin_val:\n",
    "                SVinvarRadiUn_b_list.append(SVinvarRadiUn_b.iloc[m, :])\n",
    "        else:\n",
    "            if -bound_margin_val < signa < bound_margin_val:\n",
    "                SVinvarRadiUn_b_list.append(SVinvarRadiUn_b.iloc[m, :])\n",
    "\n",
    "    SVinvarUn_b = pd.DataFrame(SVinvarRadiUn_b_list, columns=objInfoNames)\n",
    "    \n",
    "    SVinvar_orgUn_b = pd.concat([SVtotal, SVinvarUn_b])\n",
    "\n",
    "    trainFeatVSVMUn_b = SVinvar_orgUn_b.iloc[:, :-1]\n",
    "    trainLabelsVSVMUn_b = SVinvar_orgUn_b.iloc[:, -1]\n",
    "\n",
    "    countTrainDataUn_b = SVinvar_orgUn_b.shape[0]\n",
    "    indexTrainDataUn_b = [list(range(1, countTrainDataUn_b + 1))]\n",
    "\n",
    "    names = objInfoNames[:-1]\n",
    "    tuneFeatVSVMUn_b = pd.concat([trainFeatVSVMUn_b, testFeatsub], axis=0)\n",
    "    tuneFeatVSVMUn_b.columns = names\n",
    "    tuneLabelsVSVMUn_b = np.concatenate((trainLabelsVSVMUn_b.values, testLabels.values))\n",
    "\n",
    "    tunedVSVMUn_b = SVC(kernel='linear')\n",
    "    tunedVSVMUn_b.fit(tuneFeatVSVMUn_b, tuneLabelsVSVMUn_b)\n",
    "\n",
    "    if actKappa < tunedVSVMUn_b.resample.Kappa:\n",
    "        bestFittingModelUn_b = tunedVSVMUn_b\n",
    "        actKappa = tunedVSVMUn_b.resample.Kappa\n",
    "\n",
    "# Run classification and accuracy assessment for the best bound setting\n",
    "predLabelsVSVMsumUn_b = bestFittingModelUn_b.predict(validateFeatsub)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL_Un_b = accuracy_score(validateLabels, predLabelsVSVMsumUn_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted labels to the features data set\n",
    "predLabelsVSVMsumUn_unc = pd.concat([validateFeatsub, pd.DataFrame(predLabelsVSVMsumUn_b, columns=[\"Predicted_Labels\"])], axis=1)\n",
    "predLabelsVSVMsumUn_unc.columns = objInfoNames\n",
    "\n",
    "# Calculate uncertainty of the samples by selecting SV's and data set\n",
    "normdistvsvm_sl_un = uncertainty_dist_v2_2(bestFittingModelUn_b, predLabelsVSVMsumUn_unc)\n",
    "\n",
    "# Alter labels\n",
    "predlabels_vsvm_Slu = alter_labels(normdistvsvm_sl_un, validateLabels)\n",
    "\n",
    "# Accuracy assessment\n",
    "accVSVM_SL_Un_b_ad = accuracy_score(validateLabels, predlabels_vsvm_Slu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
